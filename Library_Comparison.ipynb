{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Machine Learning Libraries\n",
    "\n",
    "For this overview example, we will create a classification model using:\n",
    "\n",
    "1. scikit-learn\n",
    "2. Keras\n",
    "3. PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "CPU times: user 15.7 ms, sys: 3.2 ms, total: 18.9 ms\n",
      "Wall time: 33.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cancer = load_breast_cancer()\n",
    "X_scaled = StandardScaler().fit_transform(cancer.data)\n",
    "print(X_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 496)\n",
      "CPU times: user 14.7 ms, sys: 3.64 ms, total: 18.3 ms\n",
      "Wall time: 43.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "print(X_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 278)\n",
      "CPU times: user 11.1 s, sys: 138 ms, total: 11.2 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfc = RandomForestClassifier(max_depth=7, n_estimators=10, random_state=1)\n",
    "rfecv = RFECV(estimator=rfc, cv=5, n_jobs=-1)\n",
    "X_poly_top = rfecv.fit_transform(X_poly, cancer.target)\n",
    "print(X_poly_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937062937063\n",
      "CPU times: user 63.2 ms, sys: 5.19 ms, total: 68.4 ms\n",
      "Wall time: 71.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly_top, cancer.target, random_state=42)\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=7, n_estimators=10, random_state=1)\n",
    "print(rfc.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "in_dim = cancer.data.shape[1]\n",
    "hidden1 = X_poly_top.shape[1]\n",
    "hidden2 = 20\n",
    "out_dim = 1\n",
    "batches_in_data = X_train.shape[0]/batch_size\n",
    "epochs = int(5000/batches_in_data)\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "model_k = keras.models.Sequential([\n",
    "    keras.layers.Dense(hidden1, activation='relu', input_shape=(in_dim,)),\n",
    "    keras.layers.Dense(hidden2),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dropout(rate=0.25),\n",
    "    keras.layers.Dense(out_dim, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 278)               8618      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 20)                5580      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 14,219\n",
      "Trainable params: 14,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_k.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/375\n",
      "426/426 [==============================] - 0s - loss: 0.6313 - acc: 0.3685 - val_loss: 0.6224 - val_acc: 0.3776\n",
      "Epoch 2/375\n",
      "426/426 [==============================] - 0s - loss: 0.6246 - acc: 0.3756 - val_loss: 0.6224 - val_acc: 0.3776\n",
      "Epoch 3/375\n",
      "426/426 [==============================] - 0s - loss: 0.6240 - acc: 0.3756 - val_loss: 0.6224 - val_acc: 0.3776\n",
      "Epoch 4/375\n",
      "426/426 [==============================] - 0s - loss: 0.6103 - acc: 0.3826 - val_loss: 0.6223 - val_acc: 0.3776\n",
      "Epoch 5/375\n",
      "426/426 [==============================] - 0s - loss: 0.5357 - acc: 0.4601 - val_loss: 0.6224 - val_acc: 0.3776\n",
      "Epoch 6/375\n",
      "426/426 [==============================] - 0s - loss: 0.5165 - acc: 0.4718 - val_loss: 0.4019 - val_acc: 0.5315\n",
      "Epoch 7/375\n",
      "426/426 [==============================] - 0s - loss: 0.4576 - acc: 0.5282 - val_loss: 0.0850 - val_acc: 0.9021\n",
      "Epoch 8/375\n",
      "426/426 [==============================] - 0s - loss: 0.3832 - acc: 0.6080 - val_loss: 0.2336 - val_acc: 0.7552\n",
      "Epoch 9/375\n",
      "426/426 [==============================] - 0s - loss: 0.3596 - acc: 0.6362 - val_loss: 0.3162 - val_acc: 0.6713\n",
      "Epoch 10/375\n",
      "426/426 [==============================] - 0s - loss: 0.3628 - acc: 0.6291 - val_loss: 0.3549 - val_acc: 0.6434\n",
      "Epoch 11/375\n",
      "426/426 [==============================] - 0s - loss: 0.3730 - acc: 0.6221 - val_loss: 0.3705 - val_acc: 0.6294\n",
      "Epoch 12/375\n",
      "426/426 [==============================] - 0s - loss: 0.3060 - acc: 0.6854 - val_loss: 0.2608 - val_acc: 0.7273\n",
      "Epoch 13/375\n",
      "426/426 [==============================] - 0s - loss: 0.3490 - acc: 0.6479 - val_loss: 0.3700 - val_acc: 0.6294\n",
      "Epoch 14/375\n",
      "426/426 [==============================] - 0s - loss: 0.3252 - acc: 0.6667 - val_loss: 0.3706 - val_acc: 0.6294\n",
      "Epoch 15/375\n",
      "426/426 [==============================] - 0s - loss: 0.3609 - acc: 0.6338 - val_loss: 0.3418 - val_acc: 0.6573\n",
      "Epoch 16/375\n",
      "426/426 [==============================] - 0s - loss: 0.3629 - acc: 0.6291 - val_loss: 0.3255 - val_acc: 0.6713\n",
      "Epoch 17/375\n",
      "426/426 [==============================] - 0s - loss: 0.2744 - acc: 0.7136 - val_loss: 0.0680 - val_acc: 0.9301\n",
      "Epoch 18/375\n",
      "426/426 [==============================] - 0s - loss: 0.3478 - acc: 0.6479 - val_loss: 0.0651 - val_acc: 0.9301\n",
      "Epoch 19/375\n",
      "426/426 [==============================] - 0s - loss: 0.3177 - acc: 0.6761 - val_loss: 0.2308 - val_acc: 0.7622\n",
      "Epoch 20/375\n",
      "426/426 [==============================] - 0s - loss: 0.2955 - acc: 0.6972 - val_loss: 0.2505 - val_acc: 0.7413\n",
      "Epoch 21/375\n",
      "426/426 [==============================] - 0s - loss: 0.2842 - acc: 0.7136 - val_loss: 0.3254 - val_acc: 0.6713\n",
      "Epoch 22/375\n",
      "426/426 [==============================] - 0s - loss: 0.3159 - acc: 0.6784 - val_loss: 0.0661 - val_acc: 0.9231\n",
      "Epoch 23/375\n",
      "426/426 [==============================] - 0s - loss: 0.2793 - acc: 0.7113 - val_loss: 0.0749 - val_acc: 0.9161\n",
      "Epoch 24/375\n",
      "426/426 [==============================] - 0s - loss: 0.3161 - acc: 0.6737 - val_loss: 0.1603 - val_acc: 0.8392\n",
      "Epoch 25/375\n",
      "426/426 [==============================] - 0s - loss: 0.2607 - acc: 0.7347 - val_loss: 0.1285 - val_acc: 0.8671\n",
      "Epoch 26/375\n",
      "426/426 [==============================] - 0s - loss: 0.2938 - acc: 0.6995 - val_loss: 0.1095 - val_acc: 0.8881\n",
      "Epoch 27/375\n",
      "426/426 [==============================] - 0s - loss: 0.2535 - acc: 0.7394 - val_loss: 0.0699 - val_acc: 0.9231\n",
      "Epoch 28/375\n",
      "426/426 [==============================] - 0s - loss: 0.2761 - acc: 0.7113 - val_loss: 0.1040 - val_acc: 0.8881\n",
      "Epoch 29/375\n",
      "426/426 [==============================] - 0s - loss: 0.2607 - acc: 0.7371 - val_loss: 0.1511 - val_acc: 0.8392\n",
      "Epoch 30/375\n",
      "426/426 [==============================] - 0s - loss: 0.2341 - acc: 0.7535 - val_loss: 0.0594 - val_acc: 0.9441\n",
      "Epoch 31/375\n",
      "426/426 [==============================] - 0s - loss: 0.2532 - acc: 0.7394 - val_loss: 0.0666 - val_acc: 0.9231\n",
      "Epoch 32/375\n",
      "426/426 [==============================] - 0s - loss: 0.2289 - acc: 0.7606 - val_loss: 0.0947 - val_acc: 0.8881\n",
      "Epoch 33/375\n",
      "426/426 [==============================] - 0s - loss: 0.2117 - acc: 0.7817 - val_loss: 0.0597 - val_acc: 0.9371\n",
      "Epoch 34/375\n",
      "426/426 [==============================] - 0s - loss: 0.2082 - acc: 0.7746 - val_loss: 0.1232 - val_acc: 0.8741\n",
      "Epoch 35/375\n",
      "426/426 [==============================] - 0s - loss: 0.2199 - acc: 0.7770 - val_loss: 0.1069 - val_acc: 0.8881\n",
      "Epoch 36/375\n",
      "426/426 [==============================] - 0s - loss: 0.1913 - acc: 0.7981 - val_loss: 0.0603 - val_acc: 0.9371\n",
      "Epoch 37/375\n",
      "426/426 [==============================] - 0s - loss: 0.1880 - acc: 0.8028 - val_loss: 0.0673 - val_acc: 0.9301\n",
      "Epoch 38/375\n",
      "426/426 [==============================] - 0s - loss: 0.1549 - acc: 0.8404 - val_loss: 0.0670 - val_acc: 0.9301\n",
      "Epoch 39/375\n",
      "426/426 [==============================] - 0s - loss: 0.1972 - acc: 0.7958 - val_loss: 0.0727 - val_acc: 0.9231\n",
      "Epoch 40/375\n",
      "426/426 [==============================] - 0s - loss: 0.1566 - acc: 0.8310 - val_loss: 0.0840 - val_acc: 0.9021\n",
      "Epoch 41/375\n",
      "426/426 [==============================] - 0s - loss: 0.1608 - acc: 0.8263 - val_loss: 0.0817 - val_acc: 0.9091\n",
      "Epoch 42/375\n",
      "426/426 [==============================] - 0s - loss: 0.1447 - acc: 0.8498 - val_loss: 0.0694 - val_acc: 0.9301\n",
      "Epoch 43/375\n",
      "426/426 [==============================] - 0s - loss: 0.1563 - acc: 0.8310 - val_loss: 0.0662 - val_acc: 0.9301\n",
      "Epoch 44/375\n",
      "426/426 [==============================] - 0s - loss: 0.1647 - acc: 0.8216 - val_loss: 0.0651 - val_acc: 0.9301\n",
      "Epoch 45/375\n",
      "426/426 [==============================] - 0s - loss: 0.1667 - acc: 0.8286 - val_loss: 0.0928 - val_acc: 0.9021\n",
      "Epoch 46/375\n",
      "426/426 [==============================] - 0s - loss: 0.1362 - acc: 0.8545 - val_loss: 0.0639 - val_acc: 0.9301\n",
      "Epoch 47/375\n",
      "426/426 [==============================] - 0s - loss: 0.1297 - acc: 0.8592 - val_loss: 0.0620 - val_acc: 0.9301\n",
      "Epoch 48/375\n",
      "426/426 [==============================] - 0s - loss: 0.1167 - acc: 0.8756 - val_loss: 0.0815 - val_acc: 0.9091\n",
      "Epoch 49/375\n",
      "426/426 [==============================] - 0s - loss: 0.1440 - acc: 0.8474 - val_loss: 0.0650 - val_acc: 0.9301\n",
      "Epoch 50/375\n",
      "426/426 [==============================] - 0s - loss: 0.1392 - acc: 0.8521 - val_loss: 0.0574 - val_acc: 0.9441\n",
      "Epoch 51/375\n",
      "426/426 [==============================] - 0s - loss: 0.1382 - acc: 0.8568 - val_loss: 0.0636 - val_acc: 0.9371\n",
      "Epoch 52/375\n",
      "426/426 [==============================] - 0s - loss: 0.1439 - acc: 0.8427 - val_loss: 0.0634 - val_acc: 0.9231\n",
      "Epoch 53/375\n",
      "426/426 [==============================] - 0s - loss: 0.1178 - acc: 0.8709 - val_loss: 0.0639 - val_acc: 0.9231\n",
      "Epoch 54/375\n",
      "426/426 [==============================] - 0s - loss: 0.1262 - acc: 0.8638 - val_loss: 0.0859 - val_acc: 0.9021\n",
      "Epoch 55/375\n",
      "426/426 [==============================] - 0s - loss: 0.1454 - acc: 0.8380 - val_loss: 0.0738 - val_acc: 0.9161\n",
      "Epoch 56/375\n",
      "426/426 [==============================] - 0s - loss: 0.1064 - acc: 0.8873 - val_loss: 0.0649 - val_acc: 0.9301\n",
      "Epoch 57/375\n",
      "426/426 [==============================] - 0s - loss: 0.1125 - acc: 0.8756 - val_loss: 0.0574 - val_acc: 0.9441\n",
      "Epoch 58/375\n",
      "426/426 [==============================] - 0s - loss: 0.1114 - acc: 0.8873 - val_loss: 0.0643 - val_acc: 0.9301\n",
      "Epoch 59/375\n",
      "426/426 [==============================] - 0s - loss: 0.1305 - acc: 0.8638 - val_loss: 0.0568 - val_acc: 0.9441\n",
      "Epoch 60/375\n",
      "426/426 [==============================] - 0s - loss: 0.1156 - acc: 0.8779 - val_loss: 0.0991 - val_acc: 0.8881\n",
      "Epoch 61/375\n",
      "426/426 [==============================] - 0s - loss: 0.1561 - acc: 0.8380 - val_loss: 0.0658 - val_acc: 0.9301\n",
      "Epoch 62/375\n",
      "426/426 [==============================] - 0s - loss: 0.1186 - acc: 0.8756 - val_loss: 0.0644 - val_acc: 0.9301\n",
      "Epoch 63/375\n",
      "426/426 [==============================] - 0s - loss: 0.1225 - acc: 0.8662 - val_loss: 0.0718 - val_acc: 0.9161\n",
      "Epoch 64/375\n",
      "426/426 [==============================] - 0s - loss: 0.1203 - acc: 0.8732 - val_loss: 0.0639 - val_acc: 0.9371\n",
      "Epoch 65/375\n",
      "426/426 [==============================] - 0s - loss: 0.1171 - acc: 0.8732 - val_loss: 0.0634 - val_acc: 0.9301\n",
      "Epoch 66/375\n",
      "426/426 [==============================] - 0s - loss: 0.1018 - acc: 0.8920 - val_loss: 0.0584 - val_acc: 0.9371\n",
      "Epoch 67/375\n",
      "426/426 [==============================] - 0s - loss: 0.1167 - acc: 0.8732 - val_loss: 0.0667 - val_acc: 0.9301\n",
      "Epoch 68/375\n",
      "426/426 [==============================] - 0s - loss: 0.0965 - acc: 0.8944 - val_loss: 0.0655 - val_acc: 0.9231\n",
      "Epoch 69/375\n",
      "426/426 [==============================] - 0s - loss: 0.1050 - acc: 0.8850 - val_loss: 0.0668 - val_acc: 0.9301\n",
      "Epoch 70/375\n",
      "426/426 [==============================] - 0s - loss: 0.1284 - acc: 0.8685 - val_loss: 0.0656 - val_acc: 0.9231\n",
      "Epoch 71/375\n",
      "426/426 [==============================] - 0s - loss: 0.1090 - acc: 0.8850 - val_loss: 0.0564 - val_acc: 0.9441\n",
      "Epoch 72/375\n",
      "426/426 [==============================] - 0s - loss: 0.1078 - acc: 0.8826 - val_loss: 0.0657 - val_acc: 0.9301\n",
      "Epoch 73/375\n",
      "426/426 [==============================] - 0s - loss: 0.1073 - acc: 0.8897 - val_loss: 0.0663 - val_acc: 0.9301\n",
      "Epoch 74/375\n",
      "426/426 [==============================] - 0s - loss: 0.1180 - acc: 0.8779 - val_loss: 0.0634 - val_acc: 0.9371\n",
      "Epoch 75/375\n",
      "426/426 [==============================] - 0s - loss: 0.1381 - acc: 0.8474 - val_loss: 0.0626 - val_acc: 0.9371\n",
      "Epoch 76/375\n",
      "426/426 [==============================] - 0s - loss: 0.1082 - acc: 0.8873 - val_loss: 0.0582 - val_acc: 0.9371\n",
      "Epoch 77/375\n",
      "426/426 [==============================] - 0s - loss: 0.1091 - acc: 0.8826 - val_loss: 0.0568 - val_acc: 0.9441\n",
      "Epoch 78/375\n",
      "426/426 [==============================] - 0s - loss: 0.1224 - acc: 0.8732 - val_loss: 0.0622 - val_acc: 0.9301\n",
      "Epoch 79/375\n",
      "426/426 [==============================] - 0s - loss: 0.1239 - acc: 0.8732 - val_loss: 0.0636 - val_acc: 0.9301\n",
      "Epoch 80/375\n",
      "426/426 [==============================] - 0s - loss: 0.0991 - acc: 0.8897 - val_loss: 0.0649 - val_acc: 0.9231\n",
      "Epoch 81/375\n",
      "426/426 [==============================] - 0s - loss: 0.0985 - acc: 0.8944 - val_loss: 0.0628 - val_acc: 0.9371\n",
      "Epoch 82/375\n",
      "426/426 [==============================] - 0s - loss: 0.0996 - acc: 0.8967 - val_loss: 0.0565 - val_acc: 0.9441\n",
      "Epoch 83/375\n",
      "426/426 [==============================] - 0s - loss: 0.1026 - acc: 0.8897 - val_loss: 0.0642 - val_acc: 0.9301\n",
      "Epoch 84/375\n",
      "426/426 [==============================] - 0s - loss: 0.0996 - acc: 0.8944 - val_loss: 0.0630 - val_acc: 0.9371\n",
      "Epoch 85/375\n",
      "426/426 [==============================] - 0s - loss: 0.1078 - acc: 0.8803 - val_loss: 0.0565 - val_acc: 0.9441\n",
      "Epoch 86/375\n",
      "426/426 [==============================] - 0s - loss: 0.1102 - acc: 0.8826 - val_loss: 0.0612 - val_acc: 0.9231\n",
      "Epoch 87/375\n",
      "426/426 [==============================] - 0s - loss: 0.0991 - acc: 0.8967 - val_loss: 0.0747 - val_acc: 0.9161\n",
      "Epoch 88/375\n",
      "426/426 [==============================] - 0s - loss: 0.1116 - acc: 0.8850 - val_loss: 0.0653 - val_acc: 0.9301\n",
      "Epoch 89/375\n",
      "426/426 [==============================] - 0s - loss: 0.1066 - acc: 0.8873 - val_loss: 0.0658 - val_acc: 0.9231\n",
      "Epoch 90/375\n",
      "426/426 [==============================] - 0s - loss: 0.1008 - acc: 0.8967 - val_loss: 0.0663 - val_acc: 0.9301\n",
      "Epoch 91/375\n",
      "426/426 [==============================] - 0s - loss: 0.0941 - acc: 0.9038 - val_loss: 0.0659 - val_acc: 0.9301\n",
      "Epoch 92/375\n",
      "426/426 [==============================] - 0s - loss: 0.1107 - acc: 0.8803 - val_loss: 0.0751 - val_acc: 0.9161\n",
      "Epoch 93/375\n",
      "426/426 [==============================] - 0s - loss: 0.1038 - acc: 0.8850 - val_loss: 0.0572 - val_acc: 0.9441\n",
      "Epoch 94/375\n",
      "426/426 [==============================] - 0s - loss: 0.1000 - acc: 0.8920 - val_loss: 0.0606 - val_acc: 0.9301\n",
      "Epoch 95/375\n",
      "426/426 [==============================] - 0s - loss: 0.1024 - acc: 0.8944 - val_loss: 0.0653 - val_acc: 0.9301\n",
      "Epoch 96/375\n",
      "426/426 [==============================] - 0s - loss: 0.1008 - acc: 0.8897 - val_loss: 0.0636 - val_acc: 0.9371\n",
      "Epoch 97/375\n",
      "426/426 [==============================] - 0s - loss: 0.0938 - acc: 0.8991 - val_loss: 0.0654 - val_acc: 0.9301\n",
      "Epoch 98/375\n",
      "426/426 [==============================] - 0s - loss: 0.1061 - acc: 0.8873 - val_loss: 0.0565 - val_acc: 0.9441\n",
      "Epoch 99/375\n",
      "426/426 [==============================] - 0s - loss: 0.1034 - acc: 0.8920 - val_loss: 0.0626 - val_acc: 0.9371\n",
      "Epoch 100/375\n",
      "426/426 [==============================] - 0s - loss: 0.0925 - acc: 0.9014 - val_loss: 0.0696 - val_acc: 0.9161\n",
      "Epoch 101/375\n",
      "426/426 [==============================] - 0s - loss: 0.1097 - acc: 0.8826 - val_loss: 0.0643 - val_acc: 0.9371\n",
      "Epoch 102/375\n",
      "426/426 [==============================] - 0s - loss: 0.0974 - acc: 0.8944 - val_loss: 0.0623 - val_acc: 0.9371\n",
      "Epoch 103/375\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.1083 - acc: 0.889 - 0s - loss: 0.1081 - acc: 0.8897 - val_loss: 0.0563 - val_acc: 0.9441\n",
      "Epoch 104/375\n",
      "426/426 [==============================] - 0s - loss: 0.0862 - acc: 0.9131 - val_loss: 0.0647 - val_acc: 0.9301\n",
      "Epoch 105/375\n",
      "426/426 [==============================] - 0s - loss: 0.1158 - acc: 0.8779 - val_loss: 0.0710 - val_acc: 0.9231\n",
      "Epoch 106/375\n",
      "426/426 [==============================] - 0s - loss: 0.1153 - acc: 0.8732 - val_loss: 0.0688 - val_acc: 0.9161\n",
      "Epoch 107/375\n",
      "426/426 [==============================] - 0s - loss: 0.1049 - acc: 0.8826 - val_loss: 0.0637 - val_acc: 0.9161\n",
      "Epoch 108/375\n",
      "426/426 [==============================] - 0s - loss: 0.0990 - acc: 0.8944 - val_loss: 0.0650 - val_acc: 0.9371\n",
      "Epoch 109/375\n",
      "426/426 [==============================] - 0s - loss: 0.0872 - acc: 0.9061 - val_loss: 0.0575 - val_acc: 0.9301\n",
      "Epoch 110/375\n",
      "426/426 [==============================] - 0s - loss: 0.0892 - acc: 0.9085 - val_loss: 0.0599 - val_acc: 0.9231\n",
      "Epoch 111/375\n",
      "426/426 [==============================] - 0s - loss: 0.0875 - acc: 0.9085 - val_loss: 0.0579 - val_acc: 0.9371\n",
      "Epoch 112/375\n",
      "426/426 [==============================] - 0s - loss: 0.0969 - acc: 0.8967 - val_loss: 0.0630 - val_acc: 0.9301\n",
      "Epoch 113/375\n",
      "426/426 [==============================] - 0s - loss: 0.0962 - acc: 0.9014 - val_loss: 0.0556 - val_acc: 0.9371\n",
      "Epoch 114/375\n",
      "426/426 [==============================] - 0s - loss: 0.1007 - acc: 0.8920 - val_loss: 0.0617 - val_acc: 0.9231\n",
      "Epoch 115/375\n",
      "426/426 [==============================] - 0s - loss: 0.1089 - acc: 0.8850 - val_loss: 0.0607 - val_acc: 0.9301\n",
      "Epoch 116/375\n",
      "426/426 [==============================] - 0s - loss: 0.0879 - acc: 0.9061 - val_loss: 0.0615 - val_acc: 0.9301\n",
      "Epoch 117/375\n",
      "426/426 [==============================] - 0s - loss: 0.0967 - acc: 0.8967 - val_loss: 0.0690 - val_acc: 0.9231\n",
      "Epoch 118/375\n",
      "426/426 [==============================] - 0s - loss: 0.0980 - acc: 0.8920 - val_loss: 0.0614 - val_acc: 0.9371\n",
      "Epoch 119/375\n",
      "426/426 [==============================] - 0s - loss: 0.0918 - acc: 0.9038 - val_loss: 0.0574 - val_acc: 0.9301\n",
      "Epoch 120/375\n",
      "426/426 [==============================] - 0s - loss: 0.0983 - acc: 0.8897 - val_loss: 0.0602 - val_acc: 0.9371\n",
      "Epoch 121/375\n",
      "426/426 [==============================] - 0s - loss: 0.0999 - acc: 0.8873 - val_loss: 0.0577 - val_acc: 0.9301\n",
      "Epoch 122/375\n",
      "426/426 [==============================] - 0s - loss: 0.0942 - acc: 0.8967 - val_loss: 0.0580 - val_acc: 0.9301\n",
      "Epoch 123/375\n",
      "426/426 [==============================] - 0s - loss: 0.0836 - acc: 0.9108 - val_loss: 0.0662 - val_acc: 0.9301\n",
      "Epoch 124/375\n",
      "426/426 [==============================] - 0s - loss: 0.1014 - acc: 0.8873 - val_loss: 0.0580 - val_acc: 0.9301\n",
      "Epoch 125/375\n",
      "426/426 [==============================] - 0s - loss: 0.0909 - acc: 0.9014 - val_loss: 0.0593 - val_acc: 0.9371\n",
      "Epoch 126/375\n",
      "426/426 [==============================] - 0s - loss: 0.1071 - acc: 0.8850 - val_loss: 0.0699 - val_acc: 0.9231\n",
      "Epoch 127/375\n",
      "426/426 [==============================] - 0s - loss: 0.1012 - acc: 0.8944 - val_loss: 0.0756 - val_acc: 0.9231\n",
      "Epoch 128/375\n",
      "426/426 [==============================] - 0s - loss: 0.0976 - acc: 0.8967 - val_loss: 0.0599 - val_acc: 0.9371\n",
      "Epoch 129/375\n",
      "426/426 [==============================] - 0s - loss: 0.0902 - acc: 0.9038 - val_loss: 0.0612 - val_acc: 0.9371\n",
      "Epoch 130/375\n",
      "426/426 [==============================] - 0s - loss: 0.0851 - acc: 0.9108 - val_loss: 0.0551 - val_acc: 0.9441\n",
      "Epoch 131/375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s - loss: 0.0899 - acc: 0.9014 - val_loss: 0.0590 - val_acc: 0.9371\n",
      "Epoch 132/375\n",
      "426/426 [==============================] - 0s - loss: 0.0931 - acc: 0.9038 - val_loss: 0.0592 - val_acc: 0.9371\n",
      "Epoch 133/375\n",
      "426/426 [==============================] - 0s - loss: 0.0895 - acc: 0.9038 - val_loss: 0.0579 - val_acc: 0.9231\n",
      "Epoch 134/375\n",
      "426/426 [==============================] - 0s - loss: 0.0988 - acc: 0.8897 - val_loss: 0.0581 - val_acc: 0.9371\n",
      "Epoch 135/375\n",
      "426/426 [==============================] - 0s - loss: 0.0862 - acc: 0.9061 - val_loss: 0.0589 - val_acc: 0.9371\n",
      "Epoch 136/375\n",
      "426/426 [==============================] - 0s - loss: 0.0930 - acc: 0.9038 - val_loss: 0.0603 - val_acc: 0.9371\n",
      "Epoch 137/375\n",
      "426/426 [==============================] - 0s - loss: 0.0865 - acc: 0.9085 - val_loss: 0.0613 - val_acc: 0.9301\n",
      "Epoch 138/375\n",
      "426/426 [==============================] - 0s - loss: 0.0919 - acc: 0.8991 - val_loss: 0.0593 - val_acc: 0.9301\n",
      "Epoch 139/375\n",
      "426/426 [==============================] - 0s - loss: 0.0838 - acc: 0.9108 - val_loss: 0.0601 - val_acc: 0.9301\n",
      "Epoch 140/375\n",
      "426/426 [==============================] - 0s - loss: 0.0921 - acc: 0.8991 - val_loss: 0.0597 - val_acc: 0.9371\n",
      "Epoch 141/375\n",
      "426/426 [==============================] - 0s - loss: 0.0942 - acc: 0.8967 - val_loss: 0.0576 - val_acc: 0.9371\n",
      "Epoch 142/375\n",
      "426/426 [==============================] - 0s - loss: 0.0886 - acc: 0.9014 - val_loss: 0.0556 - val_acc: 0.9441\n",
      "Epoch 143/375\n",
      "426/426 [==============================] - 0s - loss: 0.0903 - acc: 0.9085 - val_loss: 0.0607 - val_acc: 0.9301\n",
      "Epoch 144/375\n",
      "426/426 [==============================] - 0s - loss: 0.0920 - acc: 0.8944 - val_loss: 0.0579 - val_acc: 0.9301\n",
      "Epoch 145/375\n",
      "426/426 [==============================] - 0s - loss: 0.0861 - acc: 0.9038 - val_loss: 0.0639 - val_acc: 0.9301\n",
      "Epoch 146/375\n",
      "426/426 [==============================] - 0s - loss: 0.0917 - acc: 0.8991 - val_loss: 0.0658 - val_acc: 0.9301\n",
      "Epoch 147/375\n",
      "426/426 [==============================] - 0s - loss: 0.0910 - acc: 0.9038 - val_loss: 0.0641 - val_acc: 0.9301\n",
      "Epoch 148/375\n",
      "426/426 [==============================] - 0s - loss: 0.0935 - acc: 0.8991 - val_loss: 0.0559 - val_acc: 0.9371\n",
      "Epoch 149/375\n",
      "426/426 [==============================] - 0s - loss: 0.0906 - acc: 0.9061 - val_loss: 0.0565 - val_acc: 0.9441\n",
      "Epoch 150/375\n",
      "426/426 [==============================] - 0s - loss: 0.0913 - acc: 0.8967 - val_loss: 0.0554 - val_acc: 0.9371\n",
      "Epoch 151/375\n",
      "426/426 [==============================] - 0s - loss: 0.0877 - acc: 0.9061 - val_loss: 0.0552 - val_acc: 0.9441\n",
      "Epoch 152/375\n",
      "426/426 [==============================] - 0s - loss: 0.0849 - acc: 0.9038 - val_loss: 0.0601 - val_acc: 0.9301\n",
      "Epoch 153/375\n",
      "426/426 [==============================] - 0s - loss: 0.0814 - acc: 0.9131 - val_loss: 0.0556 - val_acc: 0.9441\n",
      "Epoch 154/375\n",
      "426/426 [==============================] - 0s - loss: 0.0943 - acc: 0.8944 - val_loss: 0.0578 - val_acc: 0.9301\n",
      "Epoch 155/375\n",
      "426/426 [==============================] - 0s - loss: 0.0818 - acc: 0.9131 - val_loss: 0.0826 - val_acc: 0.9091\n",
      "Epoch 156/375\n",
      "426/426 [==============================] - 0s - loss: 0.0927 - acc: 0.9014 - val_loss: 0.0549 - val_acc: 0.9301\n",
      "Epoch 157/375\n",
      "426/426 [==============================] - 0s - loss: 0.0924 - acc: 0.9038 - val_loss: 0.0566 - val_acc: 0.9371\n",
      "Epoch 158/375\n",
      "426/426 [==============================] - 0s - loss: 0.0976 - acc: 0.8944 - val_loss: 0.0562 - val_acc: 0.9371\n",
      "Epoch 159/375\n",
      "426/426 [==============================] - 0s - loss: 0.0809 - acc: 0.9155 - val_loss: 0.0550 - val_acc: 0.9441\n",
      "Epoch 160/375\n",
      "426/426 [==============================] - 0s - loss: 0.0836 - acc: 0.9108 - val_loss: 0.0555 - val_acc: 0.9301\n",
      "Epoch 161/375\n",
      "426/426 [==============================] - 0s - loss: 0.0954 - acc: 0.8967 - val_loss: 0.0545 - val_acc: 0.9301\n",
      "Epoch 162/375\n",
      "426/426 [==============================] - 0s - loss: 0.0824 - acc: 0.9155 - val_loss: 0.0632 - val_acc: 0.9301\n",
      "Epoch 163/375\n",
      "426/426 [==============================] - 0s - loss: 0.0836 - acc: 0.9108 - val_loss: 0.0594 - val_acc: 0.9371\n",
      "Epoch 164/375\n",
      "426/426 [==============================] - 0s - loss: 0.0928 - acc: 0.9014 - val_loss: 0.0530 - val_acc: 0.9441\n",
      "Epoch 165/375\n",
      "426/426 [==============================] - 0s - loss: 0.0844 - acc: 0.9085 - val_loss: 0.0559 - val_acc: 0.9371\n",
      "Epoch 166/375\n",
      "426/426 [==============================] - 0s - loss: 0.0876 - acc: 0.9061 - val_loss: 0.0642 - val_acc: 0.9231\n",
      "Epoch 167/375\n",
      "426/426 [==============================] - 0s - loss: 0.0865 - acc: 0.9061 - val_loss: 0.0615 - val_acc: 0.9371\n",
      "Epoch 168/375\n",
      "426/426 [==============================] - 0s - loss: 0.0830 - acc: 0.9178 - val_loss: 0.0603 - val_acc: 0.9371\n",
      "Epoch 169/375\n",
      "426/426 [==============================] - 0s - loss: 0.0901 - acc: 0.9014 - val_loss: 0.0517 - val_acc: 0.9301\n",
      "Epoch 170/375\n",
      "426/426 [==============================] - 0s - loss: 0.0917 - acc: 0.8991 - val_loss: 0.0514 - val_acc: 0.9441\n",
      "Epoch 171/375\n",
      "426/426 [==============================] - 0s - loss: 0.0851 - acc: 0.9085 - val_loss: 0.0504 - val_acc: 0.9441\n",
      "Epoch 172/375\n",
      "426/426 [==============================] - 0s - loss: 0.0833 - acc: 0.9085 - val_loss: 0.0527 - val_acc: 0.9371\n",
      "Epoch 173/375\n",
      "426/426 [==============================] - 0s - loss: 0.0872 - acc: 0.9038 - val_loss: 0.0602 - val_acc: 0.9231\n",
      "Epoch 174/375\n",
      "426/426 [==============================] - 0s - loss: 0.0906 - acc: 0.8944 - val_loss: 0.0600 - val_acc: 0.9301\n",
      "Epoch 175/375\n",
      "426/426 [==============================] - 0s - loss: 0.0917 - acc: 0.9014 - val_loss: 0.0668 - val_acc: 0.9301\n",
      "Epoch 176/375\n",
      "426/426 [==============================] - 0s - loss: 0.0838 - acc: 0.9061 - val_loss: 0.0566 - val_acc: 0.9371\n",
      "Epoch 177/375\n",
      "426/426 [==============================] - 0s - loss: 0.0836 - acc: 0.9108 - val_loss: 0.0582 - val_acc: 0.9371\n",
      "Epoch 178/375\n",
      "426/426 [==============================] - 0s - loss: 0.0859 - acc: 0.9061 - val_loss: 0.0607 - val_acc: 0.9231\n",
      "Epoch 179/375\n",
      "426/426 [==============================] - 0s - loss: 0.0862 - acc: 0.9038 - val_loss: 0.0607 - val_acc: 0.9301\n",
      "Epoch 180/375\n",
      "426/426 [==============================] - 0s - loss: 0.0832 - acc: 0.9085 - val_loss: 0.0593 - val_acc: 0.9301\n",
      "Epoch 181/375\n",
      "426/426 [==============================] - 0s - loss: 0.0822 - acc: 0.9131 - val_loss: 0.0550 - val_acc: 0.9301\n",
      "Epoch 182/375\n",
      "426/426 [==============================] - 0s - loss: 0.0756 - acc: 0.9178 - val_loss: 0.0631 - val_acc: 0.9371\n",
      "Epoch 183/375\n",
      "426/426 [==============================] - 0s - loss: 0.0873 - acc: 0.9038 - val_loss: 0.0645 - val_acc: 0.9231\n",
      "Epoch 184/375\n",
      "426/426 [==============================] - 0s - loss: 0.0876 - acc: 0.9038 - val_loss: 0.0602 - val_acc: 0.9301\n",
      "Epoch 185/375\n",
      "426/426 [==============================] - 0s - loss: 0.0847 - acc: 0.9038 - val_loss: 0.0567 - val_acc: 0.9441\n",
      "Epoch 186/375\n",
      "426/426 [==============================] - 0s - loss: 0.0967 - acc: 0.9014 - val_loss: 0.0743 - val_acc: 0.9161\n",
      "Epoch 187/375\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.1136 - acc: 0.875 - 0s - loss: 0.0907 - acc: 0.9038 - val_loss: 0.0591 - val_acc: 0.9371\n",
      "Epoch 188/375\n",
      "426/426 [==============================] - 0s - loss: 0.0892 - acc: 0.9061 - val_loss: 0.0559 - val_acc: 0.9441\n",
      "Epoch 189/375\n",
      "426/426 [==============================] - 0s - loss: 0.0871 - acc: 0.9061 - val_loss: 0.0597 - val_acc: 0.9371\n",
      "Epoch 190/375\n",
      "426/426 [==============================] - 0s - loss: 0.0936 - acc: 0.8850 - val_loss: 0.0547 - val_acc: 0.9441\n",
      "Epoch 191/375\n",
      "426/426 [==============================] - 0s - loss: 0.0887 - acc: 0.9014 - val_loss: 0.0629 - val_acc: 0.9231\n",
      "Epoch 192/375\n",
      "426/426 [==============================] - 0s - loss: 0.0866 - acc: 0.9038 - val_loss: 0.0529 - val_acc: 0.9371\n",
      "Epoch 193/375\n",
      "426/426 [==============================] - 0s - loss: 0.0801 - acc: 0.9131 - val_loss: 0.0482 - val_acc: 0.9301\n",
      "Epoch 194/375\n",
      "426/426 [==============================] - 0s - loss: 0.0759 - acc: 0.9108 - val_loss: 0.0482 - val_acc: 0.9441\n",
      "Epoch 195/375\n",
      "426/426 [==============================] - 0s - loss: 0.0838 - acc: 0.9085 - val_loss: 0.0541 - val_acc: 0.9371\n",
      "Epoch 196/375\n",
      "426/426 [==============================] - 0s - loss: 0.0851 - acc: 0.9038 - val_loss: 0.0489 - val_acc: 0.9371\n",
      "Epoch 197/375\n",
      "426/426 [==============================] - 0s - loss: 0.1006 - acc: 0.8873 - val_loss: 0.0551 - val_acc: 0.9371\n",
      "Epoch 198/375\n",
      "426/426 [==============================] - 0s - loss: 0.0911 - acc: 0.8991 - val_loss: 0.0519 - val_acc: 0.9441\n",
      "Epoch 199/375\n",
      "426/426 [==============================] - 0s - loss: 0.0841 - acc: 0.9061 - val_loss: 0.0486 - val_acc: 0.9371\n",
      "Epoch 200/375\n",
      "426/426 [==============================] - 0s - loss: 0.0718 - acc: 0.9225 - val_loss: 0.0705 - val_acc: 0.9161\n",
      "Epoch 201/375\n",
      "426/426 [==============================] - 0s - loss: 0.0864 - acc: 0.9014 - val_loss: 0.0542 - val_acc: 0.9441\n",
      "Epoch 202/375\n",
      "426/426 [==============================] - 0s - loss: 0.0821 - acc: 0.9108 - val_loss: 0.0494 - val_acc: 0.9441\n",
      "Epoch 203/375\n",
      "426/426 [==============================] - 0s - loss: 0.0771 - acc: 0.9131 - val_loss: 0.0469 - val_acc: 0.9301\n",
      "Epoch 204/375\n",
      "426/426 [==============================] - 0s - loss: 0.0873 - acc: 0.8967 - val_loss: 0.0536 - val_acc: 0.9441\n",
      "Epoch 205/375\n",
      "426/426 [==============================] - 0s - loss: 0.0807 - acc: 0.9108 - val_loss: 0.0466 - val_acc: 0.9371\n",
      "Epoch 206/375\n",
      "426/426 [==============================] - 0s - loss: 0.0759 - acc: 0.9178 - val_loss: 0.0513 - val_acc: 0.9231\n",
      "Epoch 207/375\n",
      "426/426 [==============================] - 0s - loss: 0.0879 - acc: 0.9014 - val_loss: 0.0515 - val_acc: 0.9441\n",
      "Epoch 208/375\n",
      "426/426 [==============================] - 0s - loss: 0.0839 - acc: 0.9061 - val_loss: 0.0488 - val_acc: 0.9441\n",
      "Epoch 209/375\n",
      "426/426 [==============================] - 0s - loss: 0.0749 - acc: 0.9202 - val_loss: 0.0498 - val_acc: 0.9441\n",
      "Epoch 210/375\n",
      "426/426 [==============================] - 0s - loss: 0.0754 - acc: 0.9202 - val_loss: 0.0504 - val_acc: 0.9441\n",
      "Epoch 211/375\n",
      "426/426 [==============================] - 0s - loss: 0.0848 - acc: 0.9085 - val_loss: 0.0589 - val_acc: 0.9301\n",
      "Epoch 212/375\n",
      "426/426 [==============================] - 0s - loss: 0.0853 - acc: 0.9108 - val_loss: 0.0730 - val_acc: 0.9161\n",
      "Epoch 213/375\n",
      "426/426 [==============================] - 0s - loss: 0.0843 - acc: 0.9108 - val_loss: 0.0664 - val_acc: 0.9301\n",
      "Epoch 214/375\n",
      "426/426 [==============================] - 0s - loss: 0.0888 - acc: 0.9014 - val_loss: 0.0416 - val_acc: 0.9510\n",
      "Epoch 215/375\n",
      "426/426 [==============================] - 0s - loss: 0.0939 - acc: 0.8967 - val_loss: 0.0461 - val_acc: 0.9371\n",
      "Epoch 216/375\n",
      "426/426 [==============================] - 0s - loss: 0.0837 - acc: 0.9085 - val_loss: 0.0562 - val_acc: 0.9371\n",
      "Epoch 217/375\n",
      "426/426 [==============================] - 0s - loss: 0.0815 - acc: 0.9085 - val_loss: 0.0552 - val_acc: 0.9371\n",
      "Epoch 218/375\n",
      "426/426 [==============================] - 0s - loss: 0.0775 - acc: 0.9131 - val_loss: 0.0516 - val_acc: 0.9371\n",
      "Epoch 219/375\n",
      "426/426 [==============================] - 0s - loss: 0.0848 - acc: 0.9085 - val_loss: 0.0621 - val_acc: 0.9301\n",
      "Epoch 220/375\n",
      "426/426 [==============================] - 0s - loss: 0.0886 - acc: 0.9014 - val_loss: 0.0492 - val_acc: 0.9441\n",
      "Epoch 221/375\n",
      "426/426 [==============================] - 0s - loss: 0.0773 - acc: 0.9085 - val_loss: 0.0441 - val_acc: 0.9371\n",
      "Epoch 222/375\n",
      "426/426 [==============================] - 0s - loss: 0.0800 - acc: 0.9038 - val_loss: 0.0486 - val_acc: 0.9510\n",
      "Epoch 223/375\n",
      "426/426 [==============================] - 0s - loss: 0.0768 - acc: 0.9178 - val_loss: 0.0743 - val_acc: 0.9161\n",
      "Epoch 224/375\n",
      "426/426 [==============================] - 0s - loss: 0.0879 - acc: 0.8967 - val_loss: 0.0666 - val_acc: 0.9301\n",
      "Epoch 225/375\n",
      "426/426 [==============================] - 0s - loss: 0.0798 - acc: 0.9155 - val_loss: 0.0486 - val_acc: 0.9441\n",
      "Epoch 226/375\n",
      "426/426 [==============================] - 0s - loss: 0.0842 - acc: 0.9085 - val_loss: 0.0466 - val_acc: 0.9441\n",
      "Epoch 227/375\n",
      "426/426 [==============================] - 0s - loss: 0.0973 - acc: 0.8967 - val_loss: 0.0487 - val_acc: 0.9371\n",
      "Epoch 228/375\n",
      "426/426 [==============================] - 0s - loss: 0.0789 - acc: 0.9131 - val_loss: 0.0457 - val_acc: 0.9510\n",
      "Epoch 229/375\n",
      "426/426 [==============================] - 0s - loss: 0.0753 - acc: 0.9131 - val_loss: 0.0468 - val_acc: 0.9371\n",
      "Epoch 230/375\n",
      "426/426 [==============================] - 0s - loss: 0.0806 - acc: 0.9038 - val_loss: 0.0436 - val_acc: 0.9441\n",
      "Epoch 231/375\n",
      "426/426 [==============================] - 0s - loss: 0.0816 - acc: 0.8991 - val_loss: 0.0449 - val_acc: 0.9441\n",
      "Epoch 232/375\n",
      "426/426 [==============================] - 0s - loss: 0.0860 - acc: 0.8967 - val_loss: 0.0434 - val_acc: 0.9441\n",
      "Epoch 233/375\n",
      "426/426 [==============================] - 0s - loss: 0.0823 - acc: 0.9061 - val_loss: 0.0430 - val_acc: 0.9441\n",
      "Epoch 234/375\n",
      "426/426 [==============================] - 0s - loss: 0.0812 - acc: 0.9061 - val_loss: 0.0454 - val_acc: 0.9371\n",
      "Epoch 235/375\n",
      "426/426 [==============================] - 0s - loss: 0.0752 - acc: 0.9131 - val_loss: 0.0448 - val_acc: 0.9371\n",
      "Epoch 236/375\n",
      "426/426 [==============================] - 0s - loss: 0.0775 - acc: 0.9085 - val_loss: 0.0438 - val_acc: 0.9510\n",
      "Epoch 237/375\n",
      "426/426 [==============================] - 0s - loss: 0.0725 - acc: 0.9202 - val_loss: 0.0471 - val_acc: 0.9371\n",
      "Epoch 238/375\n",
      "426/426 [==============================] - 0s - loss: 0.0789 - acc: 0.9108 - val_loss: 0.0439 - val_acc: 0.9441\n",
      "Epoch 239/375\n",
      "426/426 [==============================] - 0s - loss: 0.0776 - acc: 0.9131 - val_loss: 0.0436 - val_acc: 0.9441\n",
      "Epoch 240/375\n",
      "426/426 [==============================] - 0s - loss: 0.0782 - acc: 0.9108 - val_loss: 0.0443 - val_acc: 0.9371\n",
      "Epoch 241/375\n",
      "426/426 [==============================] - 0s - loss: 0.0807 - acc: 0.9061 - val_loss: 0.0418 - val_acc: 0.9510\n",
      "Epoch 242/375\n",
      "426/426 [==============================] - 0s - loss: 0.0789 - acc: 0.9108 - val_loss: 0.0504 - val_acc: 0.9441\n",
      "Epoch 243/375\n",
      "426/426 [==============================] - 0s - loss: 0.0821 - acc: 0.9108 - val_loss: 0.0475 - val_acc: 0.9371\n",
      "Epoch 244/375\n",
      "426/426 [==============================] - 0s - loss: 0.0846 - acc: 0.9061 - val_loss: 0.0707 - val_acc: 0.9301\n",
      "Epoch 245/375\n",
      "426/426 [==============================] - 0s - loss: 0.0732 - acc: 0.9202 - val_loss: 0.0479 - val_acc: 0.9441\n",
      "Epoch 246/375\n",
      "426/426 [==============================] - 0s - loss: 0.0867 - acc: 0.9061 - val_loss: 0.0638 - val_acc: 0.9301\n",
      "Epoch 247/375\n",
      "426/426 [==============================] - 0s - loss: 0.0887 - acc: 0.9061 - val_loss: 0.0511 - val_acc: 0.9371\n",
      "Epoch 248/375\n",
      "426/426 [==============================] - 0s - loss: 0.0792 - acc: 0.9038 - val_loss: 0.0446 - val_acc: 0.9510\n",
      "Epoch 249/375\n",
      "426/426 [==============================] - 0s - loss: 0.0789 - acc: 0.9108 - val_loss: 0.0451 - val_acc: 0.9371\n",
      "Epoch 250/375\n",
      "426/426 [==============================] - 0s - loss: 0.0846 - acc: 0.9038 - val_loss: 0.0481 - val_acc: 0.9510\n",
      "Epoch 251/375\n",
      "426/426 [==============================] - 0s - loss: 0.0865 - acc: 0.8944 - val_loss: 0.0486 - val_acc: 0.9441\n",
      "Epoch 252/375\n",
      "426/426 [==============================] - 0s - loss: 0.0696 - acc: 0.9249 - val_loss: 0.0405 - val_acc: 0.9510\n",
      "Epoch 253/375\n",
      "426/426 [==============================] - 0s - loss: 0.0741 - acc: 0.9131 - val_loss: 0.0544 - val_acc: 0.9371\n",
      "Epoch 254/375\n",
      "426/426 [==============================] - 0s - loss: 0.0728 - acc: 0.9178 - val_loss: 0.0424 - val_acc: 0.9441\n",
      "Epoch 255/375\n",
      "426/426 [==============================] - 0s - loss: 0.0796 - acc: 0.9131 - val_loss: 0.0439 - val_acc: 0.9510\n",
      "Epoch 256/375\n",
      "426/426 [==============================] - 0s - loss: 0.0746 - acc: 0.9178 - val_loss: 0.0386 - val_acc: 0.9510\n",
      "Epoch 257/375\n",
      "426/426 [==============================] - 0s - loss: 0.0770 - acc: 0.9178 - val_loss: 0.0404 - val_acc: 0.9441\n",
      "Epoch 258/375\n",
      "426/426 [==============================] - 0s - loss: 0.0818 - acc: 0.9038 - val_loss: 0.0671 - val_acc: 0.9301\n",
      "Epoch 259/375\n",
      "426/426 [==============================] - 0s - loss: 0.0841 - acc: 0.9061 - val_loss: 0.0423 - val_acc: 0.9510\n",
      "Epoch 260/375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s - loss: 0.0726 - acc: 0.9155 - val_loss: 0.0415 - val_acc: 0.9510\n",
      "Epoch 261/375\n",
      "426/426 [==============================] - 0s - loss: 0.0741 - acc: 0.9131 - val_loss: 0.0543 - val_acc: 0.9301\n",
      "Epoch 262/375\n",
      "426/426 [==============================] - 0s - loss: 0.0737 - acc: 0.9225 - val_loss: 0.0434 - val_acc: 0.9441\n",
      "Epoch 263/375\n",
      "426/426 [==============================] - 0s - loss: 0.0804 - acc: 0.9061 - val_loss: 0.0424 - val_acc: 0.9510\n",
      "Epoch 264/375\n",
      "426/426 [==============================] - 0s - loss: 0.0772 - acc: 0.9108 - val_loss: 0.0601 - val_acc: 0.9231\n",
      "Epoch 265/375\n",
      "426/426 [==============================] - 0s - loss: 0.0779 - acc: 0.9202 - val_loss: 0.0503 - val_acc: 0.9441\n",
      "Epoch 266/375\n",
      "426/426 [==============================] - 0s - loss: 0.0820 - acc: 0.9038 - val_loss: 0.0400 - val_acc: 0.9510\n",
      "Epoch 267/375\n",
      "426/426 [==============================] - 0s - loss: 0.0723 - acc: 0.9202 - val_loss: 0.0408 - val_acc: 0.9441\n",
      "Epoch 268/375\n",
      "426/426 [==============================] - 0s - loss: 0.0977 - acc: 0.8803 - val_loss: 0.0393 - val_acc: 0.9441\n",
      "Epoch 269/375\n",
      "426/426 [==============================] - 0s - loss: 0.0772 - acc: 0.9131 - val_loss: 0.0560 - val_acc: 0.9301\n",
      "Epoch 270/375\n",
      "426/426 [==============================] - 0s - loss: 0.0819 - acc: 0.8944 - val_loss: 0.0385 - val_acc: 0.9510\n",
      "Epoch 271/375\n",
      "426/426 [==============================] - 0s - loss: 0.0852 - acc: 0.8991 - val_loss: 0.0430 - val_acc: 0.9441\n",
      "Epoch 272/375\n",
      "426/426 [==============================] - 0s - loss: 0.0780 - acc: 0.9038 - val_loss: 0.0360 - val_acc: 0.9510\n",
      "Epoch 273/375\n",
      "426/426 [==============================] - 0s - loss: 0.0762 - acc: 0.9178 - val_loss: 0.0352 - val_acc: 0.9580\n",
      "Epoch 274/375\n",
      "426/426 [==============================] - 0s - loss: 0.0840 - acc: 0.9038 - val_loss: 0.0355 - val_acc: 0.9580\n",
      "Epoch 275/375\n",
      "426/426 [==============================] - 0s - loss: 0.0806 - acc: 0.9155 - val_loss: 0.0501 - val_acc: 0.9371\n",
      "Epoch 276/375\n",
      "426/426 [==============================] - 0s - loss: 0.0800 - acc: 0.9085 - val_loss: 0.0448 - val_acc: 0.9441\n",
      "Epoch 277/375\n",
      "426/426 [==============================] - 0s - loss: 0.0711 - acc: 0.9202 - val_loss: 0.0460 - val_acc: 0.9441\n",
      "Epoch 278/375\n",
      "426/426 [==============================] - 0s - loss: 0.0733 - acc: 0.9178 - val_loss: 0.0354 - val_acc: 0.9510\n",
      "Epoch 279/375\n",
      "426/426 [==============================] - 0s - loss: 0.0829 - acc: 0.9014 - val_loss: 0.0358 - val_acc: 0.9510\n",
      "Epoch 280/375\n",
      "426/426 [==============================] - 0s - loss: 0.0764 - acc: 0.9108 - val_loss: 0.0368 - val_acc: 0.9510\n",
      "Epoch 281/375\n",
      "426/426 [==============================] - 0s - loss: 0.0749 - acc: 0.9155 - val_loss: 0.0403 - val_acc: 0.9441\n",
      "Epoch 282/375\n",
      "426/426 [==============================] - 0s - loss: 0.0772 - acc: 0.9108 - val_loss: 0.0380 - val_acc: 0.9510\n",
      "Epoch 283/375\n",
      "426/426 [==============================] - 0s - loss: 0.0694 - acc: 0.9155 - val_loss: 0.0552 - val_acc: 0.9371\n",
      "Epoch 284/375\n",
      "426/426 [==============================] - 0s - loss: 0.0720 - acc: 0.9202 - val_loss: 0.0411 - val_acc: 0.9371\n",
      "Epoch 285/375\n",
      "426/426 [==============================] - 0s - loss: 0.0745 - acc: 0.9178 - val_loss: 0.0403 - val_acc: 0.9441\n",
      "Epoch 286/375\n",
      "426/426 [==============================] - 0s - loss: 0.0813 - acc: 0.9014 - val_loss: 0.0401 - val_acc: 0.9441\n",
      "Epoch 287/375\n",
      "426/426 [==============================] - 0s - loss: 0.0645 - acc: 0.9249 - val_loss: 0.0430 - val_acc: 0.9441\n",
      "Epoch 288/375\n",
      "426/426 [==============================] - 0s - loss: 0.0726 - acc: 0.9178 - val_loss: 0.0360 - val_acc: 0.9510\n",
      "Epoch 289/375\n",
      "426/426 [==============================] - 0s - loss: 0.0736 - acc: 0.9108 - val_loss: 0.0356 - val_acc: 0.9580\n",
      "Epoch 290/375\n",
      "426/426 [==============================] - 0s - loss: 0.0734 - acc: 0.9178 - val_loss: 0.0370 - val_acc: 0.9510\n",
      "Epoch 291/375\n",
      "426/426 [==============================] - 0s - loss: 0.0772 - acc: 0.9155 - val_loss: 0.0374 - val_acc: 0.9441\n",
      "Epoch 292/375\n",
      "426/426 [==============================] - 0s - loss: 0.0676 - acc: 0.9249 - val_loss: 0.0374 - val_acc: 0.9510\n",
      "Epoch 293/375\n",
      "426/426 [==============================] - 0s - loss: 0.0724 - acc: 0.9178 - val_loss: 0.0396 - val_acc: 0.9510\n",
      "Epoch 294/375\n",
      "426/426 [==============================] - 0s - loss: 0.0732 - acc: 0.9178 - val_loss: 0.0357 - val_acc: 0.9580\n",
      "Epoch 295/375\n",
      "426/426 [==============================] - 0s - loss: 0.0670 - acc: 0.9272 - val_loss: 0.0398 - val_acc: 0.9510\n",
      "Epoch 296/375\n",
      "426/426 [==============================] - 0s - loss: 0.0777 - acc: 0.9131 - val_loss: 0.0408 - val_acc: 0.9510\n",
      "Epoch 297/375\n",
      "426/426 [==============================] - 0s - loss: 0.0791 - acc: 0.9155 - val_loss: 0.0748 - val_acc: 0.9091\n",
      "Epoch 298/375\n",
      "426/426 [==============================] - 0s - loss: 0.0811 - acc: 0.9014 - val_loss: 0.0406 - val_acc: 0.9510\n",
      "Epoch 299/375\n",
      "426/426 [==============================] - 0s - loss: 0.0701 - acc: 0.9108 - val_loss: 0.0402 - val_acc: 0.9441\n",
      "Epoch 300/375\n",
      "426/426 [==============================] - 0s - loss: 0.0733 - acc: 0.9225 - val_loss: 0.0421 - val_acc: 0.9441\n",
      "Epoch 301/375\n",
      "426/426 [==============================] - 0s - loss: 0.0727 - acc: 0.9178 - val_loss: 0.0389 - val_acc: 0.9371\n",
      "Epoch 302/375\n",
      "426/426 [==============================] - 0s - loss: 0.0700 - acc: 0.9108 - val_loss: 0.0364 - val_acc: 0.9510\n",
      "Epoch 303/375\n",
      "426/426 [==============================] - 0s - loss: 0.0746 - acc: 0.9108 - val_loss: 0.0770 - val_acc: 0.9161\n",
      "Epoch 304/375\n",
      "426/426 [==============================] - 0s - loss: 0.0684 - acc: 0.9249 - val_loss: 0.0373 - val_acc: 0.9510\n",
      "Epoch 305/375\n",
      "426/426 [==============================] - 0s - loss: 0.0768 - acc: 0.9108 - val_loss: 0.0384 - val_acc: 0.9510\n",
      "Epoch 306/375\n",
      "426/426 [==============================] - 0s - loss: 0.0695 - acc: 0.9178 - val_loss: 0.0463 - val_acc: 0.9441\n",
      "Epoch 307/375\n",
      "426/426 [==============================] - 0s - loss: 0.0746 - acc: 0.9155 - val_loss: 0.0379 - val_acc: 0.9371\n",
      "Epoch 308/375\n",
      "426/426 [==============================] - 0s - loss: 0.0711 - acc: 0.9202 - val_loss: 0.0450 - val_acc: 0.9510\n",
      "Epoch 309/375\n",
      "426/426 [==============================] - 0s - loss: 0.0719 - acc: 0.9178 - val_loss: 0.0374 - val_acc: 0.9441\n",
      "Epoch 310/375\n",
      "426/426 [==============================] - 0s - loss: 0.0673 - acc: 0.9085 - val_loss: 0.0701 - val_acc: 0.9231\n",
      "Epoch 311/375\n",
      "426/426 [==============================] - 0s - loss: 0.0667 - acc: 0.9202 - val_loss: 0.0554 - val_acc: 0.9301\n",
      "Epoch 312/375\n",
      "426/426 [==============================] - 0s - loss: 0.0765 - acc: 0.9061 - val_loss: 0.0390 - val_acc: 0.9510\n",
      "Epoch 313/375\n",
      "426/426 [==============================] - 0s - loss: 0.0691 - acc: 0.9225 - val_loss: 0.0473 - val_acc: 0.9441\n",
      "Epoch 314/375\n",
      "426/426 [==============================] - 0s - loss: 0.0760 - acc: 0.9108 - val_loss: 0.0399 - val_acc: 0.9510\n",
      "Epoch 315/375\n",
      "426/426 [==============================] - 0s - loss: 0.0685 - acc: 0.9178 - val_loss: 0.0534 - val_acc: 0.9301\n",
      "Epoch 316/375\n",
      "426/426 [==============================] - 0s - loss: 0.0618 - acc: 0.9225 - val_loss: 0.0381 - val_acc: 0.9510\n",
      "Epoch 317/375\n",
      "426/426 [==============================] - 0s - loss: 0.0831 - acc: 0.9038 - val_loss: 0.0480 - val_acc: 0.9371\n",
      "Epoch 318/375\n",
      "426/426 [==============================] - 0s - loss: 0.0744 - acc: 0.9108 - val_loss: 0.0468 - val_acc: 0.9371\n",
      "Epoch 319/375\n",
      "426/426 [==============================] - 0s - loss: 0.0713 - acc: 0.9131 - val_loss: 0.0403 - val_acc: 0.9580\n",
      "Epoch 320/375\n",
      "426/426 [==============================] - 0s - loss: 0.0633 - acc: 0.9202 - val_loss: 0.0365 - val_acc: 0.9510\n",
      "Epoch 321/375\n",
      "426/426 [==============================] - 0s - loss: 0.0710 - acc: 0.9131 - val_loss: 0.0924 - val_acc: 0.9021\n",
      "Epoch 322/375\n",
      "426/426 [==============================] - 0s - loss: 0.0702 - acc: 0.9155 - val_loss: 0.0457 - val_acc: 0.9441\n",
      "Epoch 323/375\n",
      "426/426 [==============================] - 0s - loss: 0.0671 - acc: 0.9178 - val_loss: 0.0330 - val_acc: 0.9580\n",
      "Epoch 324/375\n",
      "426/426 [==============================] - 0s - loss: 0.0715 - acc: 0.9085 - val_loss: 0.0411 - val_acc: 0.9441\n",
      "Epoch 325/375\n",
      "426/426 [==============================] - 0s - loss: 0.0665 - acc: 0.9249 - val_loss: 0.0348 - val_acc: 0.9441\n",
      "Epoch 326/375\n",
      "426/426 [==============================] - 0s - loss: 0.0839 - acc: 0.9014 - val_loss: 0.0411 - val_acc: 0.9510\n",
      "Epoch 327/375\n",
      "426/426 [==============================] - 0s - loss: 0.0654 - acc: 0.9225 - val_loss: 0.0400 - val_acc: 0.9441\n",
      "Epoch 328/375\n",
      "426/426 [==============================] - 0s - loss: 0.0726 - acc: 0.9085 - val_loss: 0.0384 - val_acc: 0.9510\n",
      "Epoch 329/375\n",
      "426/426 [==============================] - 0s - loss: 0.0669 - acc: 0.9155 - val_loss: 0.0605 - val_acc: 0.9371\n",
      "Epoch 330/375\n",
      "426/426 [==============================] - 0s - loss: 0.0719 - acc: 0.9178 - val_loss: 0.0459 - val_acc: 0.9441\n",
      "Epoch 331/375\n",
      "426/426 [==============================] - 0s - loss: 0.0708 - acc: 0.9178 - val_loss: 0.0571 - val_acc: 0.9231\n",
      "Epoch 332/375\n",
      "426/426 [==============================] - 0s - loss: 0.0708 - acc: 0.9178 - val_loss: 0.0717 - val_acc: 0.9161\n",
      "Epoch 333/375\n",
      "426/426 [==============================] - 0s - loss: 0.0614 - acc: 0.9319 - val_loss: 0.0334 - val_acc: 0.9510\n",
      "Epoch 334/375\n",
      "426/426 [==============================] - 0s - loss: 0.0672 - acc: 0.9202 - val_loss: 0.0588 - val_acc: 0.9371\n",
      "Epoch 335/375\n",
      "426/426 [==============================] - 0s - loss: 0.0643 - acc: 0.9249 - val_loss: 0.0815 - val_acc: 0.9161\n",
      "Epoch 336/375\n",
      "426/426 [==============================] - 0s - loss: 0.0717 - acc: 0.9202 - val_loss: 0.0392 - val_acc: 0.9580\n",
      "Epoch 337/375\n",
      "426/426 [==============================] - 0s - loss: 0.0608 - acc: 0.9296 - val_loss: 0.0589 - val_acc: 0.9371\n",
      "Epoch 338/375\n",
      "426/426 [==============================] - 0s - loss: 0.0721 - acc: 0.9202 - val_loss: 0.0388 - val_acc: 0.9441\n",
      "Epoch 339/375\n",
      "426/426 [==============================] - 0s - loss: 0.0696 - acc: 0.9178 - val_loss: 0.0371 - val_acc: 0.9510\n",
      "Epoch 340/375\n",
      "426/426 [==============================] - 0s - loss: 0.0609 - acc: 0.9272 - val_loss: 0.0576 - val_acc: 0.9371\n",
      "Epoch 341/375\n",
      "426/426 [==============================] - 0s - loss: 0.0718 - acc: 0.9131 - val_loss: 0.0407 - val_acc: 0.9510\n",
      "Epoch 342/375\n",
      "426/426 [==============================] - 0s - loss: 0.0593 - acc: 0.9343 - val_loss: 0.0580 - val_acc: 0.9371\n",
      "Epoch 343/375\n",
      "426/426 [==============================] - 0s - loss: 0.0740 - acc: 0.9202 - val_loss: 0.0302 - val_acc: 0.9580\n",
      "Epoch 344/375\n",
      "426/426 [==============================] - 0s - loss: 0.0633 - acc: 0.9272 - val_loss: 0.0453 - val_acc: 0.9441\n",
      "Epoch 345/375\n",
      "426/426 [==============================] - 0s - loss: 0.0636 - acc: 0.9225 - val_loss: 0.0363 - val_acc: 0.9580\n",
      "Epoch 346/375\n",
      "426/426 [==============================] - 0s - loss: 0.0653 - acc: 0.9225 - val_loss: 0.0334 - val_acc: 0.9510\n",
      "Epoch 347/375\n",
      "426/426 [==============================] - 0s - loss: 0.0626 - acc: 0.9249 - val_loss: 0.0787 - val_acc: 0.9161\n",
      "Epoch 348/375\n",
      "426/426 [==============================] - 0s - loss: 0.0669 - acc: 0.9202 - val_loss: 0.0329 - val_acc: 0.9580\n",
      "Epoch 349/375\n",
      "426/426 [==============================] - 0s - loss: 0.0702 - acc: 0.9155 - val_loss: 0.0827 - val_acc: 0.9161\n",
      "Epoch 350/375\n",
      "426/426 [==============================] - 0s - loss: 0.0685 - acc: 0.9178 - val_loss: 0.0736 - val_acc: 0.9091\n",
      "Epoch 351/375\n",
      "426/426 [==============================] - 0s - loss: 0.0757 - acc: 0.9038 - val_loss: 0.0494 - val_acc: 0.9371\n",
      "Epoch 352/375\n",
      "426/426 [==============================] - 0s - loss: 0.0735 - acc: 0.9155 - val_loss: 0.0370 - val_acc: 0.9650\n",
      "Epoch 353/375\n",
      "426/426 [==============================] - 0s - loss: 0.0599 - acc: 0.9319 - val_loss: 0.0511 - val_acc: 0.9371\n",
      "Epoch 354/375\n",
      "426/426 [==============================] - 0s - loss: 0.0605 - acc: 0.9272 - val_loss: 0.0940 - val_acc: 0.9021\n",
      "Epoch 355/375\n",
      "426/426 [==============================] - 0s - loss: 0.0827 - acc: 0.9131 - val_loss: 0.0375 - val_acc: 0.9441\n",
      "Epoch 356/375\n",
      "426/426 [==============================] - 0s - loss: 0.0627 - acc: 0.9296 - val_loss: 0.0313 - val_acc: 0.9510\n",
      "Epoch 357/375\n",
      "426/426 [==============================] - 0s - loss: 0.0708 - acc: 0.9108 - val_loss: 0.0414 - val_acc: 0.9441\n",
      "Epoch 358/375\n",
      "426/426 [==============================] - 0s - loss: 0.0707 - acc: 0.9155 - val_loss: 0.0414 - val_acc: 0.9371\n",
      "Epoch 359/375\n",
      "426/426 [==============================] - 0s - loss: 0.0590 - acc: 0.9272 - val_loss: 0.0369 - val_acc: 0.9441\n",
      "Epoch 360/375\n",
      "426/426 [==============================] - 0s - loss: 0.0680 - acc: 0.9202 - val_loss: 0.0570 - val_acc: 0.9371\n",
      "Epoch 361/375\n",
      "426/426 [==============================] - 0s - loss: 0.0684 - acc: 0.9178 - val_loss: 0.0483 - val_acc: 0.9371\n",
      "Epoch 362/375\n",
      "426/426 [==============================] - 0s - loss: 0.0646 - acc: 0.9202 - val_loss: 0.0550 - val_acc: 0.9371\n",
      "Epoch 363/375\n",
      "426/426 [==============================] - 0s - loss: 0.0557 - acc: 0.9202 - val_loss: 0.0642 - val_acc: 0.9301\n",
      "Epoch 364/375\n",
      "426/426 [==============================] - 0s - loss: 0.0718 - acc: 0.9155 - val_loss: 0.0506 - val_acc: 0.9371\n",
      "Epoch 365/375\n",
      "426/426 [==============================] - 0s - loss: 0.0562 - acc: 0.9366 - val_loss: 0.0471 - val_acc: 0.9371\n",
      "Epoch 366/375\n",
      "426/426 [==============================] - 0s - loss: 0.0646 - acc: 0.9225 - val_loss: 0.0449 - val_acc: 0.9371\n",
      "Epoch 367/375\n",
      "426/426 [==============================] - 0s - loss: 0.0727 - acc: 0.9178 - val_loss: 0.0406 - val_acc: 0.9510\n",
      "Epoch 368/375\n",
      "426/426 [==============================] - 0s - loss: 0.0585 - acc: 0.9319 - val_loss: 0.0425 - val_acc: 0.9441\n",
      "Epoch 369/375\n",
      "426/426 [==============================] - 0s - loss: 0.0602 - acc: 0.9296 - val_loss: 0.0664 - val_acc: 0.9231\n",
      "Epoch 370/375\n",
      "426/426 [==============================] - 0s - loss: 0.0620 - acc: 0.9249 - val_loss: 0.0326 - val_acc: 0.9441\n",
      "Epoch 371/375\n",
      "426/426 [==============================] - 0s - loss: 0.0652 - acc: 0.9249 - val_loss: 0.0517 - val_acc: 0.9371\n",
      "Epoch 372/375\n",
      "426/426 [==============================] - 0s - loss: 0.0750 - acc: 0.9108 - val_loss: 0.0293 - val_acc: 0.9580\n",
      "Epoch 373/375\n",
      "426/426 [==============================] - 0s - loss: 0.0670 - acc: 0.9249 - val_loss: 0.0356 - val_acc: 0.9441\n",
      "Epoch 374/375\n",
      "426/426 [==============================] - 0s - loss: 0.0560 - acc: 0.9343 - val_loss: 0.0439 - val_acc: 0.9441\n",
      "Epoch 375/375\n",
      "426/426 [==============================] - 0s - loss: 0.0589 - acc: 0.9296 - val_loss: 0.0342 - val_acc: 0.9371\n",
      " 32/143 [=====>........................] - ETA: 0sTest loss: 0.0341721722348\n",
      "Test accuracy: 0.937062937897\n"
     ]
    }
   ],
   "source": [
    "model_k.compile(loss='mean_squared_error',\n",
    "                optimizer=keras.optimizers.RMSprop(lr=learning_rate),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model_k.fit(X_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      verbose=True,\n",
    "                      validation_data=(X_test, y_test))\n",
    "score = model_k.evaluate(X_test, y_test, verbose=True)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=42)\n",
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential NN\n",
    "batch_size = 32\n",
    "in_dim = cancer.data.shape[1]\n",
    "hidden1 = X_poly_top.shape[1]\n",
    "hidden2 = 20\n",
    "out_dim = 1\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model_t = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_dim, hidden1),\n",
    "    torch.nn.ReLU(),\n",
    "    # Add a hidden layer that loosely represents the poly features\n",
    "    torch.nn.Linear(hidden1, hidden2),\n",
    "    torch.nn.LeakyReLU(),  ## Use LeakyReLU might limit dead neurons\n",
    "    # A Dropout layer sometimes reduces co-adaptation of neurons\n",
    "    torch.nn.Dropout(p=0.25),\n",
    "    # Add a second hidden layer for further abstraction\n",
    "    torch.nn.Linear(hidden2, out_dim),  \n",
    "    # Add an output layer\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.RMSprop(model_t.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 278]           8,618\n",
      "              ReLU-2               [-1, 1, 278]               0\n",
      "            Linear-3                [-1, 1, 20]           5,580\n",
      "         LeakyReLU-4                [-1, 1, 20]               0\n",
      "           Dropout-5                [-1, 1, 20]               0\n",
      "            Linear-6                 [-1, 1, 1]              21\n",
      "           Sigmoid-7                 [-1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 14,219\n",
      "Trainable params: 14,219\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 0.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model_t, input_size=(1,in_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 278)               8618      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 20)                5580      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 14,219\n",
      "Trainable params: 14,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0000 | Training Loss: 246.87 | Train accuracy: 0.3967 | Test accuracy: 0.4266\n",
      "Batch: 0075 | Training Loss: 148.80 | Train accuracy: 0.6479 | Test accuracy: 0.6364\n",
      "Batch: 0150 | Training Loss: 150.97 | Train accuracy: 0.6432 | Test accuracy: 0.6434\n",
      "Batch: 0225 | Training Loss: 149.40 | Train accuracy: 0.6502 | Test accuracy: 0.6503\n",
      "Batch: 0300 | Training Loss: 143.48 | Train accuracy: 0.6620 | Test accuracy: 0.6224\n",
      "Batch: 0375 | Training Loss:  33.58 | Train accuracy: 0.9108 | Test accuracy: 0.9580\n",
      "Batch: 0450 | Training Loss:  28.26 | Train accuracy: 0.9202 | Test accuracy: 0.9510\n",
      "Batch: 0525 | Training Loss:  27.62 | Train accuracy: 0.9202 | Test accuracy: 0.9441\n",
      "Batch: 0600 | Training Loss:  29.57 | Train accuracy: 0.9108 | Test accuracy: 0.9510\n",
      "Batch: 0675 | Training Loss:  25.97 | Train accuracy: 0.9296 | Test accuracy: 0.9371\n",
      "Batch: 0750 | Training Loss:  24.36 | Train accuracy: 0.9272 | Test accuracy: 0.9510\n",
      "Batch: 0825 | Training Loss:  24.89 | Train accuracy: 0.9202 | Test accuracy: 0.9441\n",
      "Batch: 0900 | Training Loss:  24.36 | Train accuracy: 0.9202 | Test accuracy: 0.9580\n",
      "Batch: 0975 | Training Loss:  23.59 | Train accuracy: 0.9155 | Test accuracy: 0.9510\n",
      "Batch: 1050 | Training Loss:  24.30 | Train accuracy: 0.9249 | Test accuracy: 0.9580\n",
      "Batch: 1125 | Training Loss:  25.73 | Train accuracy: 0.9249 | Test accuracy: 0.9371\n",
      "Batch: 1200 | Training Loss:  22.84 | Train accuracy: 0.9296 | Test accuracy: 0.9441\n",
      "Batch: 1275 | Training Loss:  21.53 | Train accuracy: 0.9343 | Test accuracy: 0.9510\n",
      "Batch: 1350 | Training Loss:  23.08 | Train accuracy: 0.9296 | Test accuracy: 0.9441\n",
      "Batch: 1425 | Training Loss:  28.59 | Train accuracy: 0.9225 | Test accuracy: 0.9301\n",
      "Batch: 1500 | Training Loss:  21.91 | Train accuracy: 0.9366 | Test accuracy: 0.9580\n",
      "Batch: 1575 | Training Loss:  20.44 | Train accuracy: 0.9390 | Test accuracy: 0.9441\n",
      "Batch: 1650 | Training Loss:  23.74 | Train accuracy: 0.9343 | Test accuracy: 0.9301\n",
      "Batch: 1725 | Training Loss:  21.50 | Train accuracy: 0.9366 | Test accuracy: 0.9580\n",
      "Batch: 1800 | Training Loss:  20.82 | Train accuracy: 0.9413 | Test accuracy: 0.9371\n",
      "Batch: 1875 | Training Loss:  20.01 | Train accuracy: 0.9343 | Test accuracy: 0.9441\n",
      "Batch: 1950 | Training Loss:  24.58 | Train accuracy: 0.9296 | Test accuracy: 0.9231\n",
      "Batch: 2025 | Training Loss:  20.35 | Train accuracy: 0.9343 | Test accuracy: 0.9441\n",
      "Batch: 2100 | Training Loss:  18.69 | Train accuracy: 0.9366 | Test accuracy: 0.9301\n",
      "Batch: 2175 | Training Loss:  19.23 | Train accuracy: 0.9507 | Test accuracy: 0.9580\n",
      "Batch: 2250 | Training Loss:  18.80 | Train accuracy: 0.9484 | Test accuracy: 0.9371\n",
      "Batch: 2325 | Training Loss:  19.04 | Train accuracy: 0.9366 | Test accuracy: 0.9580\n",
      "Batch: 2400 | Training Loss:  21.22 | Train accuracy: 0.9272 | Test accuracy: 0.9441\n",
      "Batch: 2475 | Training Loss:  18.06 | Train accuracy: 0.9484 | Test accuracy: 0.9441\n",
      "Batch: 2550 | Training Loss:  18.31 | Train accuracy: 0.9507 | Test accuracy: 0.9231\n",
      "Batch: 2625 | Training Loss:  20.55 | Train accuracy: 0.9460 | Test accuracy: 0.9371\n",
      "Batch: 2700 | Training Loss:  18.78 | Train accuracy: 0.9413 | Test accuracy: 0.9441\n",
      "Batch: 2775 | Training Loss:  17.20 | Train accuracy: 0.9531 | Test accuracy: 0.9510\n",
      "Batch: 2850 | Training Loss:  21.13 | Train accuracy: 0.9319 | Test accuracy: 0.9441\n",
      "Batch: 2925 | Training Loss:  19.66 | Train accuracy: 0.9390 | Test accuracy: 0.9510\n",
      "Batch: 3000 | Training Loss:  19.00 | Train accuracy: 0.9437 | Test accuracy: 0.9441\n",
      "Batch: 3075 | Training Loss:  17.25 | Train accuracy: 0.9531 | Test accuracy: 0.9580\n",
      "Batch: 3150 | Training Loss:  16.82 | Train accuracy: 0.9507 | Test accuracy: 0.9580\n",
      "Batch: 3225 | Training Loss:  16.11 | Train accuracy: 0.9531 | Test accuracy: 0.9580\n",
      "Batch: 3300 | Training Loss:  16.92 | Train accuracy: 0.9484 | Test accuracy: 0.9441\n",
      "Batch: 3375 | Training Loss:  17.23 | Train accuracy: 0.9484 | Test accuracy: 0.9510\n",
      "Batch: 3450 | Training Loss:  16.73 | Train accuracy: 0.9460 | Test accuracy: 0.9441\n",
      "Batch: 3525 | Training Loss:  17.23 | Train accuracy: 0.9554 | Test accuracy: 0.9580\n",
      "Batch: 3600 | Training Loss:  19.19 | Train accuracy: 0.9484 | Test accuracy: 0.9371\n",
      "Batch: 3675 | Training Loss:  16.61 | Train accuracy: 0.9460 | Test accuracy: 0.9441\n",
      "Batch: 3750 | Training Loss:  18.15 | Train accuracy: 0.9507 | Test accuracy: 0.9441\n",
      "Batch: 3825 | Training Loss:  17.30 | Train accuracy: 0.9507 | Test accuracy: 0.9441\n",
      "Batch: 3900 | Training Loss:  16.86 | Train accuracy: 0.9531 | Test accuracy: 0.9441\n",
      "Batch: 3975 | Training Loss:  15.85 | Train accuracy: 0.9484 | Test accuracy: 0.9441\n",
      "Batch: 4050 | Training Loss:  18.07 | Train accuracy: 0.9484 | Test accuracy: 0.9231\n",
      "Batch: 4125 | Training Loss:  16.15 | Train accuracy: 0.9554 | Test accuracy: 0.9441\n",
      "Batch: 4200 | Training Loss:  17.59 | Train accuracy: 0.9460 | Test accuracy: 0.9510\n",
      "Batch: 4275 | Training Loss:  15.55 | Train accuracy: 0.9531 | Test accuracy: 0.9510\n",
      "Batch: 4350 | Training Loss:  16.08 | Train accuracy: 0.9460 | Test accuracy: 0.9371\n",
      "Batch: 4425 | Training Loss:  15.54 | Train accuracy: 0.9531 | Test accuracy: 0.9510\n",
      "Batch: 4500 | Training Loss:  18.75 | Train accuracy: 0.9484 | Test accuracy: 0.9301\n",
      "Batch: 4575 | Training Loss:  15.20 | Train accuracy: 0.9577 | Test accuracy: 0.9441\n",
      "Batch: 4650 | Training Loss:  15.59 | Train accuracy: 0.9554 | Test accuracy: 0.9580\n",
      "Batch: 4725 | Training Loss:  18.26 | Train accuracy: 0.9507 | Test accuracy: 0.9231\n",
      "Batch: 4800 | Training Loss:  13.48 | Train accuracy: 0.9624 | Test accuracy: 0.9441\n",
      "Batch: 4875 | Training Loss:  17.57 | Train accuracy: 0.9460 | Test accuracy: 0.9371\n",
      "Batch: 4950 | Training Loss:  16.06 | Train accuracy: 0.9507 | Test accuracy: 0.9510\n"
     ]
    }
   ],
   "source": [
    "## Now run model\n",
    "X = torch.from_numpy(X_train).float()\n",
    "y = torch.from_numpy(y_train[:, np.newaxis]).float()\n",
    "X_test_T = torch.from_numpy(X_test).float()\n",
    "y_test_T = torch.from_numpy(y_test[:, np.newaxis]).float()\n",
    "\n",
    "show_every = 75\n",
    "for t in range(5000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model_t(X)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if not t % show_every:\n",
    "        y_test_pred = model_t(Variable(X_test_T))\n",
    "        prediction = [int(x > 0.5) for x in y_test_pred.data.numpy()]\n",
    "        test_accuracy = (prediction == y_test).sum() / len(y_test)\n",
    "        train_pred = [int(x > 0.5) for x in y_pred.data.numpy()]\n",
    "        train_accuracy = (train_pred == y_train).sum() / len(y_train)\n",
    "        print(\"Batch: %04d | Training Loss: %6.2f | Train accuracy: %.4f | Test accuracy: %.4f\" % (\n",
    "                      t, loss.item(), train_accuracy, test_accuracy))\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
