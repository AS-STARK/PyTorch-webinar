{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PyTorch\n",
    "\n",
    "* Tensors and NumPy interfaces\n",
    "* Autograd\n",
    "* Using GPUs with `torch.cuda`\n",
    "* Parallelizing on clusters with `torch.distributed`\n",
    "* Create a neural network with `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and NumPy interfaces\n",
    "\n",
    "At a first pass, PyTorch tensors are very similar to NumPy arrays.  Both are ways of storing multi-dimensional data efficiently, and much of the same \"vectorized\" style of operation applies to both. Broadcasting and elementwise operations are similar.  Moreover, many of the same functions and methods exist in both PyTorch and NumPy, and conversion between the two formats is made straightforward by PyTorch.\n",
    "\n",
    "Where PyTorch tensors go beyond NumPy arrays, and are needed for neural networks are in a couple key areas.  As a not-so-minor matter, tensors can work transparently on GPUs as well as CPUs, and this can often vastly speed up operations.  NumPy does not build in that capability, but a number of projects allow this particular capability to be used outside of PyTorch, in varying ways (see [PyCUDA](https://documen.tician.de/pycuda/array.html), [Numba](https://numba.pydata.org/numba-doc/dev/cuda/index.html), [CuPy](https://cupy.chainer.org/), [MXNet](https://mxnet.incubator.apache.org/versions/master/tutorials/basic/ndarray.html), and probably others).\n",
    "\n",
    "You *could* use PyTorch simply to work with array computation on GPUs, but what is more likely to bring you here is an equally essential capability that is not present in those other mentioned libraries (by design): Autograd.  By storing the gradients from every operation (where autograd is enabled for any component tensors), PyTorch provides reverse automatic differentiation.  That is to say, it gives you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors.  We explain this more below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random 2-D tensor\n",
      "tensor([[ 0.5615, -1.1130, -1.0479],\n",
      "        [-0.6518, -0.3695, -1.1789],\n",
      "        [ 0.4406,  0.3704, -0.1163]], requires_grad=True)\n",
      "\n",
      "Random 2-D shifted by 7, multiplied by 5\n",
      "tensor([[37.8076, 29.4348, 29.7605],\n",
      "        [31.7411, 33.1524, 29.1055],\n",
      "        [37.2028, 36.8518, 34.4184]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "print(\"Random 2-D tensor\")\n",
    "print(x)\n",
    "\n",
    "# Perform an operation on tensor\n",
    "y = (x + 7) * 5\n",
    "print(\"\\nRandom 2-D shifted by 7, multiplied by 5\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5., 10., 15.],\n",
       "         [20., 25., 30.],\n",
       "         [35., 40., 45.]]), None)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]], dtype=torch.float)\n",
    "\n",
    "# grad_fn is derivative, so offset does not matter to slope\n",
    "y.grad_fn(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplied by 3\n",
      "tensor([[113.4228,  88.3044,  89.2815],\n",
      "        [ 95.2233,  99.4572,  87.3164],\n",
      "        [111.6083, 110.5554, 103.2551]], grad_fn=<MulBackward0>)\n",
      "\n",
      "Mean of values\n",
      "tensor(99.8249, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Perform an additional vectorized operation, then a reduction\n",
    "z = y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(\"Multiplied by 3\")\n",
    "print(z)\n",
    "print(\"\\nMean of values\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --> <MeanBackward1 object at 0x11df8d0b8>\n",
      "   --> <MulBackward0 object at 0x11d4d9a20>\n",
      "    --> <MulBackward0 object at 0x11df82cf8>\n",
      "     --> <AddBackward0 object at 0x11df8d240>\n",
      "      --> <AccumulateGrad object at 0x11df8d358>\n"
     ]
    }
   ],
   "source": [
    "grad = out.grad_fn\n",
    "indent = 1\n",
    "while True:\n",
    "    print(\" \"*indent, \"-->\", grad)\n",
    "    if not grad.next_functions:\n",
    "        break\n",
    "    grad = grad.next_functions[0][0]\n",
    "    indent += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30092430.0\n",
      "1 25721254.0\n",
      "2 22510530.0\n",
      "3 18227932.0\n",
      "4 13278302.0\n",
      "5 8774677.0\n",
      "6 5520158.0\n",
      "7 3467485.5\n",
      "8 2272035.25\n",
      "9 1582319.0\n",
      "10 1173364.375\n",
      "11 915979.875\n",
      "12 742447.75\n",
      "13 617237.875\n",
      "14 522056.9375\n",
      "15 446764.28125\n",
      "16 385746.03125\n",
      "17 335264.25\n",
      "18 293016.1875\n",
      "19 257211.3125\n",
      "20 226642.8125\n",
      "21 200417.609375\n",
      "22 177765.890625\n",
      "23 158109.1875\n",
      "24 141002.0625\n",
      "25 126028.1953125\n",
      "26 112872.28125\n",
      "27 101297.484375\n",
      "28 91083.59375\n",
      "29 82051.4296875\n",
      "30 74049.109375\n",
      "31 66931.484375\n",
      "32 60588.95703125\n",
      "33 54926.48046875\n",
      "34 49858.8203125\n",
      "35 45316.52734375\n",
      "36 41237.953125\n",
      "37 37568.44140625\n",
      "38 34263.0234375\n",
      "39 31282.5859375\n",
      "40 28595.404296875\n",
      "41 26169.27734375\n",
      "42 23972.61328125\n",
      "43 21978.310546875\n",
      "44 20168.7109375\n",
      "45 18523.453125\n",
      "46 17025.548828125\n",
      "47 15660.7734375\n",
      "48 14415.55859375\n",
      "49 13279.970703125\n",
      "50 12243.6318359375\n",
      "51 11295.333984375\n",
      "52 10427.42578125\n",
      "53 9632.2724609375\n",
      "54 8902.85546875\n",
      "55 8233.1533203125\n",
      "56 7617.99658203125\n",
      "57 7053.326171875\n",
      "58 6533.890625\n",
      "59 6055.77978515625\n",
      "60 5615.45263671875\n",
      "61 5209.8203125\n",
      "62 4835.705078125\n",
      "63 4490.38037109375\n",
      "64 4171.58544921875\n",
      "65 3876.958740234375\n",
      "66 3604.5419921875\n",
      "67 3352.654296875\n",
      "68 3119.5302734375\n",
      "69 2903.718017578125\n",
      "70 2704.029296875\n",
      "71 2518.916748046875\n",
      "72 2347.32568359375\n",
      "73 2188.106689453125\n",
      "74 2040.304443359375\n",
      "75 1903.1328125\n",
      "76 1775.7216796875\n",
      "77 1657.352294921875\n",
      "78 1547.401611328125\n",
      "79 1445.088623046875\n",
      "80 1349.8974609375\n",
      "81 1261.3428955078125\n",
      "82 1178.88330078125\n",
      "83 1102.1121826171875\n",
      "84 1030.602294921875\n",
      "85 963.9491577148438\n",
      "86 901.8706665039062\n",
      "87 843.949462890625\n",
      "88 789.9274291992188\n",
      "89 739.545654296875\n",
      "90 692.5195922851562\n",
      "91 648.597412109375\n",
      "92 607.5820922851562\n",
      "93 569.2797241210938\n",
      "94 533.5361328125\n",
      "95 500.120361328125\n",
      "96 468.894775390625\n",
      "97 439.6862487792969\n",
      "98 412.37017822265625\n",
      "99 386.82391357421875\n",
      "100 362.91851806640625\n",
      "101 340.541259765625\n",
      "102 319.60137939453125\n",
      "103 299.9856262207031\n",
      "104 281.6217346191406\n",
      "105 264.41455078125\n",
      "106 248.24151611328125\n",
      "107 233.08311462402344\n",
      "108 218.88394165039062\n",
      "109 205.5935821533203\n",
      "110 193.1188507080078\n",
      "111 181.42514038085938\n",
      "112 170.46067810058594\n",
      "113 160.18165588378906\n",
      "114 150.5398406982422\n",
      "115 141.49032592773438\n",
      "116 133.00733947753906\n",
      "117 125.04314422607422\n",
      "118 117.56971740722656\n",
      "119 110.55287170410156\n",
      "120 103.9678955078125\n",
      "121 97.7850570678711\n",
      "122 91.97782135009766\n",
      "123 86.52716827392578\n",
      "124 81.40361785888672\n",
      "125 76.59230041503906\n",
      "126 72.07379913330078\n",
      "127 67.82595825195312\n",
      "128 63.83451461791992\n",
      "129 60.08384704589844\n",
      "130 56.560020446777344\n",
      "131 53.2454719543457\n",
      "132 50.128379821777344\n",
      "133 47.19763946533203\n",
      "134 44.442073822021484\n",
      "135 41.851741790771484\n",
      "136 39.41544723510742\n",
      "137 37.1250114440918\n",
      "138 34.96817398071289\n",
      "139 32.939178466796875\n",
      "140 31.030784606933594\n",
      "141 29.23417091369629\n",
      "142 27.544219970703125\n",
      "143 25.954727172851562\n",
      "144 24.45871925354004\n",
      "145 23.049623489379883\n",
      "146 21.72269058227539\n",
      "147 20.473554611206055\n",
      "148 19.29759979248047\n",
      "149 18.190643310546875\n",
      "150 17.148868560791016\n",
      "151 16.166522979736328\n",
      "152 15.24193286895752\n",
      "153 14.37072467803955\n",
      "154 13.550675392150879\n",
      "155 12.777837753295898\n",
      "156 12.049971580505371\n",
      "157 11.363788604736328\n",
      "158 10.717782020568848\n",
      "159 10.108550071716309\n",
      "160 9.534712791442871\n",
      "161 8.994229316711426\n",
      "162 8.48455810546875\n",
      "163 8.004376411437988\n",
      "164 7.5513739585876465\n",
      "165 7.124985694885254\n",
      "166 6.72227144241333\n",
      "167 6.342676162719727\n",
      "168 5.98527717590332\n",
      "169 5.64819860458374\n",
      "170 5.330111503601074\n",
      "171 5.030594825744629\n",
      "172 4.747654438018799\n",
      "173 4.4811506271362305\n",
      "174 4.229510307312012\n",
      "175 3.9926235675811768\n",
      "176 3.7687883377075195\n",
      "177 3.5577425956726074\n",
      "178 3.3588814735412598\n",
      "179 3.171088218688965\n",
      "180 2.99407696723938\n",
      "181 2.826786756515503\n",
      "182 2.6691458225250244\n",
      "183 2.5202934741973877\n",
      "184 2.3798775672912598\n",
      "185 2.247519016265869\n",
      "186 2.1225414276123047\n",
      "187 2.0045430660247803\n",
      "188 1.8931212425231934\n",
      "189 1.7880619764328003\n",
      "190 1.6888880729675293\n",
      "191 1.595197319984436\n",
      "192 1.5068037509918213\n",
      "193 1.423411250114441\n",
      "194 1.3445671796798706\n",
      "195 1.270256519317627\n",
      "196 1.2000595331192017\n",
      "197 1.133749008178711\n",
      "198 1.0712473392486572\n",
      "199 1.0121455192565918\n",
      "200 0.9563653469085693\n",
      "201 0.903617799282074\n",
      "202 0.8539219498634338\n",
      "203 0.8069261312484741\n",
      "204 0.7625249028205872\n",
      "205 0.7206003665924072\n",
      "206 0.6810213923454285\n",
      "207 0.6436097025871277\n",
      "208 0.608268141746521\n",
      "209 0.5750420689582825\n",
      "210 0.5435211062431335\n",
      "211 0.5136585831642151\n",
      "212 0.4855431318283081\n",
      "213 0.4589783847332001\n",
      "214 0.4339081048965454\n",
      "215 0.4102001190185547\n",
      "216 0.38777872920036316\n",
      "217 0.3665924668312073\n",
      "218 0.34664055705070496\n",
      "219 0.327722430229187\n",
      "220 0.309861958026886\n",
      "221 0.2929924726486206\n",
      "222 0.2769911587238312\n",
      "223 0.2619157135486603\n",
      "224 0.24771855771541595\n",
      "225 0.23423385620117188\n",
      "226 0.22154127061367035\n",
      "227 0.2094862014055252\n",
      "228 0.1981339454650879\n",
      "229 0.18739627301692963\n",
      "230 0.1771923303604126\n",
      "231 0.16761891543865204\n",
      "232 0.15854917466640472\n",
      "233 0.14999623596668243\n",
      "234 0.14184163510799408\n",
      "235 0.13418230414390564\n",
      "236 0.12691904604434967\n",
      "237 0.12008894979953766\n",
      "238 0.1135956346988678\n",
      "239 0.10747780650854111\n",
      "240 0.10167443752288818\n",
      "241 0.09618338197469711\n",
      "242 0.09099525213241577\n",
      "243 0.08610746264457703\n",
      "244 0.08145423978567123\n",
      "245 0.0770835429430008\n",
      "246 0.07294445484876633\n",
      "247 0.0689939558506012\n",
      "248 0.06530801951885223\n",
      "249 0.061792366206645966\n",
      "250 0.05848671495914459\n",
      "251 0.055326662957668304\n",
      "252 0.05235978960990906\n",
      "253 0.04956527799367905\n",
      "254 0.04690732806921005\n",
      "255 0.044403791427612305\n",
      "256 0.042042456567287445\n",
      "257 0.039788275957107544\n",
      "258 0.037673868238925934\n",
      "259 0.035661209374666214\n",
      "260 0.033759281039237976\n",
      "261 0.03194545954465866\n",
      "262 0.030250201001763344\n",
      "263 0.02863304316997528\n",
      "264 0.027121327817440033\n",
      "265 0.025688529014587402\n",
      "266 0.02432302199304104\n",
      "267 0.02303040213882923\n",
      "268 0.021809663623571396\n",
      "269 0.020659083500504494\n",
      "270 0.019561726599931717\n",
      "271 0.01853334717452526\n",
      "272 0.017550945281982422\n",
      "273 0.016626376658678055\n",
      "274 0.01575186848640442\n",
      "275 0.014928177930414677\n",
      "276 0.01414421945810318\n",
      "277 0.013401346281170845\n",
      "278 0.012697018682956696\n",
      "279 0.012032766826450825\n",
      "280 0.011404450051486492\n",
      "281 0.010813803412020206\n",
      "282 0.010252542793750763\n",
      "283 0.009724855422973633\n",
      "284 0.009217435494065285\n",
      "285 0.008744224905967712\n",
      "286 0.008288807235658169\n",
      "287 0.00786341167986393\n",
      "288 0.007461898494511843\n",
      "289 0.007080404087901115\n",
      "290 0.006718043703585863\n",
      "291 0.006373909767717123\n",
      "292 0.006051054690033197\n",
      "293 0.0057478733360767365\n",
      "294 0.0054561118595302105\n",
      "295 0.005184072069823742\n",
      "296 0.004926525522023439\n",
      "297 0.00467638298869133\n",
      "298 0.004444644786417484\n",
      "299 0.004224165342748165\n",
      "300 0.004017400089651346\n",
      "301 0.0038225031457841396\n",
      "302 0.0036383301485329866\n",
      "303 0.003460243809968233\n",
      "304 0.003288838779553771\n",
      "305 0.003132184734568\n",
      "306 0.00298324809409678\n",
      "307 0.0028399680741131306\n",
      "308 0.002703402191400528\n",
      "309 0.0025770526845008135\n",
      "310 0.002454396104440093\n",
      "311 0.002340204082429409\n",
      "312 0.0022314584348350763\n",
      "313 0.0021286201663315296\n",
      "314 0.0020281828474253416\n",
      "315 0.001936982269398868\n",
      "316 0.0018485939363017678\n",
      "317 0.0017634945688769221\n",
      "318 0.0016834340058267117\n",
      "319 0.001610595267266035\n",
      "320 0.0015388181200250983\n",
      "321 0.0014689493691548705\n",
      "322 0.00140636065043509\n",
      "323 0.0013429748360067606\n",
      "324 0.0012838838156312704\n",
      "325 0.0012281732633709908\n",
      "326 0.0011746736709028482\n",
      "327 0.0011246218346059322\n",
      "328 0.0010786675848066807\n",
      "329 0.001030871644616127\n",
      "330 0.0009875178802758455\n",
      "331 0.0009465922485105693\n",
      "332 0.0009065294289030135\n",
      "333 0.0008695746073499322\n",
      "334 0.000834559672512114\n",
      "335 0.000800842884927988\n",
      "336 0.0007677165558561683\n",
      "337 0.000738154340069741\n",
      "338 0.0007077197660692036\n",
      "339 0.0006796316592954099\n",
      "340 0.000653107010293752\n",
      "341 0.0006274874904192984\n",
      "342 0.0006028211209923029\n",
      "343 0.0005807371344417334\n",
      "344 0.0005590112996287644\n",
      "345 0.0005385459517128766\n",
      "346 0.0005184770561754704\n",
      "347 0.0004996104980818927\n",
      "348 0.0004816316068172455\n",
      "349 0.00046449602814391255\n",
      "350 0.00044708276982419193\n",
      "351 0.0004309440846554935\n",
      "352 0.0004159134696237743\n",
      "353 0.00040074330172501504\n",
      "354 0.00038704200414940715\n",
      "355 0.00037395479739643633\n",
      "356 0.00036015434307046235\n",
      "357 0.00034858519211411476\n",
      "358 0.00033713600714690983\n",
      "359 0.0003253642644267529\n",
      "360 0.00031435341225005686\n",
      "361 0.00030426631565205753\n",
      "362 0.0002940320409834385\n",
      "363 0.00028514154837466776\n",
      "364 0.0002746327081695199\n",
      "365 0.00026618223637342453\n",
      "366 0.00025774139794521034\n",
      "367 0.0002497359528206289\n",
      "368 0.00024145292991306633\n",
      "369 0.0002338840567972511\n",
      "370 0.00022694173094350845\n",
      "371 0.00022002530749887228\n",
      "372 0.00021312353783287108\n",
      "373 0.0002067496970994398\n",
      "374 0.00020101258996874094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 0.00019402771431487054\n",
      "376 0.0001885614765342325\n",
      "377 0.0001830369874369353\n",
      "378 0.00017752006533555686\n",
      "379 0.0001724722096696496\n",
      "380 0.00016776005213614553\n",
      "381 0.0001630315964575857\n",
      "382 0.0001583377452334389\n",
      "383 0.00015381410776171833\n",
      "384 0.00014971845666877925\n",
      "385 0.0001458066690247506\n",
      "386 0.00014156801626086235\n",
      "387 0.00013760363799519837\n",
      "388 0.00013435866276267916\n",
      "389 0.000130480169900693\n",
      "390 0.00012713923933915794\n",
      "391 0.00012387083552312106\n",
      "392 0.00012071116361767054\n",
      "393 0.00011767202522605658\n",
      "394 0.0001146420108852908\n",
      "395 0.00011160575377289206\n",
      "396 0.00010900406050495803\n",
      "397 0.00010604845738271251\n",
      "398 0.00010361430031480268\n",
      "399 0.00010104291141033173\n",
      "400 9.868395864032209e-05\n",
      "401 9.610589768271893e-05\n",
      "402 9.365149162476882e-05\n",
      "403 9.127856174018234e-05\n",
      "404 8.924103167373687e-05\n",
      "405 8.698945021023974e-05\n",
      "406 8.472665649605915e-05\n",
      "407 8.327760588144884e-05\n",
      "408 8.126563625410199e-05\n",
      "409 7.921575888758525e-05\n",
      "410 7.762937457300723e-05\n",
      "411 7.59475224185735e-05\n",
      "412 7.40181640139781e-05\n",
      "413 7.234410441014916e-05\n",
      "414 7.082148658810183e-05\n",
      "415 6.908662180649117e-05\n",
      "416 6.789726467104629e-05\n",
      "417 6.627359834965318e-05\n",
      "418 6.473613029811531e-05\n",
      "419 6.37119374005124e-05\n",
      "420 6.265347474254668e-05\n",
      "421 6.117544398875907e-05\n",
      "422 5.995314859319478e-05\n",
      "423 5.871281246072613e-05\n",
      "424 5.743738802266307e-05\n",
      "425 5.635908019030467e-05\n",
      "426 5.52799720026087e-05\n",
      "427 5.4133393859956414e-05\n",
      "428 5.341798168956302e-05\n",
      "429 5.203927867114544e-05\n",
      "430 5.1018723752349615e-05\n",
      "431 5.0244241720065475e-05\n",
      "432 4.93751467729453e-05\n",
      "433 4.854138751397841e-05\n",
      "434 4.765416088048369e-05\n",
      "435 4.6931530960137025e-05\n",
      "436 4.5907028834335506e-05\n",
      "437 4.501979856286198e-05\n",
      "438 4.3982756324112415e-05\n",
      "439 4.315871046856046e-05\n",
      "440 4.238090696162544e-05\n",
      "441 4.187057857052423e-05\n",
      "442 4.092077142558992e-05\n",
      "443 4.051729047205299e-05\n",
      "444 3.974771243520081e-05\n",
      "445 3.8837490137666464e-05\n",
      "446 3.827584077953361e-05\n",
      "447 3.763330460060388e-05\n",
      "448 3.717744766618125e-05\n",
      "449 3.650757571449503e-05\n",
      "450 3.582742647267878e-05\n",
      "451 3.527172520989552e-05\n",
      "452 3.4616983612068e-05\n",
      "453 3.420583743718453e-05\n",
      "454 3.366588498465717e-05\n",
      "455 3.318598464829847e-05\n",
      "456 3.269185981480405e-05\n",
      "457 3.230506627005525e-05\n",
      "458 3.17908707074821e-05\n",
      "459 3.131711491732858e-05\n",
      "460 3.0724306270712987e-05\n",
      "461 3.0318145945784636e-05\n",
      "462 2.9949807867524214e-05\n",
      "463 2.95693153020693e-05\n",
      "464 2.9065335183986463e-05\n",
      "465 2.864911948563531e-05\n",
      "466 2.821708403644152e-05\n",
      "467 2.802086601150222e-05\n",
      "468 2.758495247690007e-05\n",
      "469 2.7304929972160608e-05\n",
      "470 2.6775393052957952e-05\n",
      "471 2.6326262741349638e-05\n",
      "472 2.5885747163556516e-05\n",
      "473 2.5431934773223475e-05\n",
      "474 2.5104174710577354e-05\n",
      "475 2.4698660126887262e-05\n",
      "476 2.446409962431062e-05\n",
      "477 2.419546763121616e-05\n",
      "478 2.3864042304921895e-05\n",
      "479 2.35416810028255e-05\n",
      "480 2.330790266569238e-05\n",
      "481 2.3230160877574235e-05\n",
      "482 2.3010230506770313e-05\n",
      "483 2.276224950037431e-05\n",
      "484 2.2437292500399053e-05\n",
      "485 2.2231317416299134e-05\n",
      "486 2.2039488612790592e-05\n",
      "487 2.167401362385135e-05\n",
      "488 2.129992026311811e-05\n",
      "489 2.111124194925651e-05\n",
      "490 2.0904642951791175e-05\n",
      "491 2.0600311472662725e-05\n",
      "492 2.0336448869784363e-05\n",
      "493 2.0033892724313773e-05\n",
      "494 1.984616756089963e-05\n",
      "495 1.9660796169773676e-05\n",
      "496 1.9376564523554407e-05\n",
      "497 1.9165145204169676e-05\n",
      "498 1.8962058675242588e-05\n",
      "499 1.873181463452056e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, dtype=torch.float, requires_grad=False)\n",
    "y = torch.randn(N, D_out, dtype=torch.float, requires_grad=False)\n",
    "\n",
    "w1 = torch.randn(D_in, H, dtype=torch.float, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs with `torch.cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing on clusters with `torch.distributed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a neural network with `torch.nn`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
